{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afe494f",
   "metadata": {},
   "source": [
    "# Understanding PushWorld PLR Training - Step by Step\n",
    "\n",
    "This notebook breaks down the PushWorld PLR (Prioritized Level Replay) training pipeline into small, atomic components so you can understand exactly how each piece works.\n",
    "\n",
    "## Overview of PLR\n",
    "\n",
    "PLR is a curriculum learning method that:\n",
    "1. **Generates random levels** (Domain Randomization / DR)\n",
    "2. **Scores levels** based on how challenging they are for the current agent\n",
    "3. **Stores high-scoring levels** in a replay buffer\n",
    "4. **Replays** stored levels to train the agent on difficult scenarios\n",
    "\n",
    "The key insight: train on levels that are neither too easy nor too hard - levels where the agent is still learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618dbc46",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "First, let's import all the necessary libraries. We'll explain what each one does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861fc138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "JAX devices: [CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import sys  # noqa\n",
    "from enum import IntEnum  # noqa\n",
    "from typing import Sequence, Tuple  # noqa\n",
    "\n",
    "# JAX ecosystem - for hardware-accelerated numerical computing\n",
    "import jax  # noqa\n",
    "import jax.numpy as jnp  # noqa\n",
    "import numpy as np  # noqa\n",
    "\n",
    "# Flax - neural network library built on JAX\n",
    "import flax.linen as nn  # noqa\n",
    "from flax import core, struct  # noqa\n",
    "from flax.linen.initializers import constant, orthogonal  # noqa\n",
    "from flax.training.train_state import TrainState as BaseTrainState  # noqa\n",
    "\n",
    "# Optax - optimization library for JAX\n",
    "import optax  # noqa\n",
    "\n",
    "# Distrax - probability distributions for JAX\n",
    "import distrax  # noqa\n",
    "\n",
    "# Type checking\n",
    "import chex  # noqa\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# PushWorld environment components\n",
    "from jaxued.environments.pushworld import (  # noqa\n",
    "    Actions,  # noqa\n",
    "    EnvParams,  # noqa\n",
    "    EnvState,  # noqa\n",
    "    Level,  # noqa\n",
    "    Observation,  # noqa\n",
    "    PushWorld,  # noqa\n",
    "    PushWorldRenderer,  # noqa\n",
    "    make_level_generator,  # noqa\n",
    "    make_level_mutator_minimax,  # noqa\n",
    "    prefabs,\n",
    ")  # noqa\n",
    "\n",
    "# PLR components\n",
    "from jaxued.level_sampler import LevelSampler  # noqa\n",
    "from jaxued.linen import ResetRNN  # noqa\n",
    "from jaxued.utils import compute_max_returns, max_mc, positive_value_loss  # noqa\n",
    "from jaxued.wrappers import AutoReplayWrapper  # noqa\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt  # noqa\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66516080",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Instead of using Hydra, we'll define our config as a simple dictionary. This makes it easier to understand what each parameter controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738363a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n",
      "  Grid size: 10x10\n",
      "  Parallel envs: 32\n",
      "  Rollout length: 256 steps\n",
      "  Replay probability: 0.8\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    # === ENVIRONMENT CONFIG ===\n",
    "    \"grid_size\": 10,  # PushWorld uses a 10x10 grid\n",
    "    \"penalize_time\": True,  # Small negative reward per step (-0.01)\n",
    "    \"reward_shaping\": False,  # Don't give intermediate rewards for goal progress\n",
    "    \"n_walls\": 10,  # Number of walls in randomly generated levels\n",
    "    \"n_movables\": 1,  # Number of movable objects (1-4)\n",
    "    \"max_steps_in_episode\": 100,  # Max steps before episode terminates\n",
    "    # === PPO HYPERPARAMETERS ===\n",
    "    \"lr\": 1e-4,  # Learning rate\n",
    "    \"max_grad_norm\": 0.5,  # Gradient clipping threshold\n",
    "    \"num_steps\": 256,  # Rollout length (steps per environment before update)\n",
    "    \"num_train_envs\": 32,  # Number of parallel environments (REDUCED for notebook)\n",
    "    \"num_minibatches\": 1,  # Number of minibatches per update\n",
    "    \"gamma\": 0.995,  # Discount factor\n",
    "    \"epoch_ppo\": 5,  # Number of PPO epochs per update\n",
    "    \"clip_eps\": 0.2,  # PPO clipping epsilon\n",
    "    \"gae_lambda\": 0.98,  # GAE lambda parameter\n",
    "    \"entropy_coeff\": 1e-3,  # Entropy bonus coefficient\n",
    "    \"critic_coeff\": 0.5,  # Value loss coefficient\n",
    "    # === PLR HYPERPARAMETERS ===\n",
    "    \"score_function\": \"MaxMC\",  # How to score levels: \"MaxMC\" or \"pvl\"\n",
    "    \"exploratory_grad_updates\": False,  # Update on DR levels? (Robust PLR = False)\n",
    "    \"level_buffer_capacity\": 4000,  # Max levels in replay buffer\n",
    "    \"replay_prob\": 0.8,  # Probability of replaying vs generating new levels\n",
    "    \"staleness_coeff\": 0.3,  # Weight for level staleness in sampling\n",
    "    \"temperature\": 0.3,  # Temperature for score-based sampling\n",
    "    \"topk_k\": 4,  # K for top-k prioritization\n",
    "    \"minimum_fill_ratio\": 0.5,  # Min buffer fill before replay starts\n",
    "    \"prioritization\": \"rank\",  # \"rank\" or \"topk\"\n",
    "    \"buffer_duplicate_check\": True,  # Check for duplicate levels\n",
    "    # === ACCEL (optional) ===\n",
    "    \"use_accel\": False,  # Use ACCEL (level mutation)?\n",
    "    \"num_edits\": 5,  # Number of mutations per level\n",
    "    # === TRAINING ===\n",
    "    \"seed\": 42,\n",
    "    \"num_updates\": 30000,  # Total training updates\n",
    "    \"eval_freq\": 250,  # Evaluate every N updates\n",
    "    \"eval_num_attempts\": 10,  # Number of eval attempts per level\n",
    "    \"eval_levels\": [  # Levels to evaluate on\n",
    "        \"TrivialPush\",\n",
    "        \"SimplePush\",\n",
    "        \"TwoGoals\",\n",
    "        \"ChainPush\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"  Grid size: {config['grid_size']}x{config['grid_size']}\")\n",
    "print(f\"  Parallel envs: {config['num_train_envs']}\")\n",
    "print(f\"  Rollout length: {config['num_steps']} steps\")\n",
    "print(f\"  Replay probability: {config['replay_prob']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb9e0c",
   "metadata": {},
   "source": [
    "## 3. Understanding the PushWorld Environment\n",
    "\n",
    "Let's first understand what the PushWorld environment looks like and how it works.\n",
    "\n",
    "### 3.1 The Environment\n",
    "\n",
    "PushWorld is a puzzle game where:\n",
    "- An **agent** (green) can move in 4 directions\n",
    "- The agent can **push** objects\n",
    "- **Movable objects** (red/blue) need to be pushed onto **goals** (light red)\n",
    "- The puzzle is solved when all movables are on their corresponding goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02262549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PushWorld Environment\n",
      "========================================\n",
      "Grid size: 10x10\n",
      "Action space: <gymnax.environments.spaces.Discrete object at 0x164e5ecf0>\n",
      "Observation space: <gymnax.environments.spaces.Box object at 0x164e5ecf0>\n",
      "Max steps per episode: 100\n",
      "\n",
      "Actions:\n",
      "  0: up\n",
      "  1: right\n",
      "  2: down\n",
      "  3: left\n"
     ]
    }
   ],
   "source": [
    "# Create the PushWorld environment\n",
    "env = PushWorld(\n",
    "    penalize_time=config[\"penalize_time\"],\n",
    "    reward_shaping=config[\"reward_shaping\"],\n",
    ")\n",
    "\n",
    "# Create environment parameters\n",
    "env_params = EnvParams(max_steps_in_episode=config[\"max_steps_in_episode\"])\n",
    "\n",
    "# Create renderer for visualization\n",
    "renderer = PushWorldRenderer(env, tile_size=32, render_grid_lines=True)\n",
    "\n",
    "print(\"PushWorld Environment\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Grid size: {env.grid_size}x{env.grid_size}\")\n",
    "print(f\"Action space: {env.action_space(env_params)}\")\n",
    "print(f\"Observation space: {env.observation_space(env_params)}\")\n",
    "print(f\"Max steps per episode: {env_params.max_steps_in_episode}\")\n",
    "print()\n",
    "print(\"Actions:\")\n",
    "for action in Actions:\n",
    "    print(f\"  {action.value}: {action.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfd7b7",
   "metadata": {},
   "source": [
    "### 3.2 Visualizing a Prefab Level\n",
    "\n",
    "Let's load and visualize one of the built-in \"prefab\" puzzle levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d72b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: SimplePush\n",
      "========================================\n",
      "W  W  W  W  W  W  W  W  W  W\n",
      "W  W  W  W  W  W  W  W  W  W\n",
      "W  W  .  .  .  .  .  .  W  W\n",
      "W  W  .  A  .  .  .  .  W  W\n",
      "W  W  .  W  W  W  .  .  W  W\n",
      "W  W  .  .  M1  .  .  .  W  W\n",
      "W  W  .  .  G1  .  .  .  W  W\n",
      "W  W  .  .  .  .  .  .  W  W\n",
      "W  W  W  W  W  W  W  W  W  W\n",
      "W  W  W  W  W  W  W  W  W  W\n",
      "\n",
      "Level structure (pytree leaves):\n",
      "  agent_pos shape: (3, 2) - up to 3 pixels per object\n",
      "  m1_pos shape: (3, 2) - movable object 1\n",
      "  g1_pos shape: (3, 2) - goal 1\n",
      "  wall_map shape: (10, 10) - 10x10 boolean grid\n",
      "\n",
      "Actual positions (x, y), -1 means unused:\n",
      "  Agent: [[ 3  3]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      "  M1: [[ 4  5]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n",
      "  G1: [[ 4  6]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n"
     ]
    }
   ],
   "source": [
    "# Load a simple prefab level\n",
    "level = Level.from_str(prefabs[\"SimplePush\"])\n",
    "\n",
    "print(\"Level: SimplePush\")\n",
    "print(\"=\" * 40)\n",
    "print(level.to_str())\n",
    "print()\n",
    "print(\"Level structure (pytree leaves):\")\n",
    "print(f\"  agent_pos shape: {level.agent_pos.shape} - up to 3 pixels per object\")\n",
    "print(f\"  m1_pos shape: {level.m1_pos.shape} - movable object 1\")\n",
    "print(f\"  g1_pos shape: {level.g1_pos.shape} - goal 1\")\n",
    "print(\n",
    "    f\"  wall_map shape: {level.wall_map.shape} - {config['grid_size']}x{config['grid_size']} boolean grid\"\n",
    ")\n",
    "print()\n",
    "print(\"Actual positions (x, y), -1 means unused:\")\n",
    "print(f\"  Agent: {level.agent_pos}\")\n",
    "print(f\"  M1: {level.m1_pos}\")\n",
    "print(f\"  G1: {level.g1_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9418527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation shape: (10, 10, 8)\n",
      "Channels: [agent, m1, m2, m3, m4, g1, g2, wall]\n"
     ]
    }
   ],
   "source": [
    "# Visualize the level\n",
    "rng = jax.random.PRNGKey(0)\n",
    "obs, state = env.reset_env_to_level(rng, level, env_params)\n",
    "\n",
    "# Render and display\n",
    "img = renderer.render_state(state, env_params)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Left: rendered image\n",
    "axes[0].imshow(np.array(img))\n",
    "axes[0].set_title(\"Rendered Level\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Right: observation channels\n",
    "# Observation is 10x10x8 - let's visualize the channels\n",
    "obs_img = obs.image\n",
    "axes[1].imshow(obs_img[:, :, 0], cmap=\"Greens\")  # Agent channel\n",
    "axes[1].set_title(\"Observation (Agent Channel)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nObservation shape: {obs.image.shape}\")\n",
    "print(\"Channels: [agent, m1, m2, m3, m4, g1, g2, wall]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef97da8",
   "metadata": {},
   "source": [
    "### 3.3 Random Level Generation\n",
    "\n",
    "For PLR, we need to generate random levels. This is called \"Domain Randomization\" (DR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fa968c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Level generator config:\n",
      "  n_walls: 10\n",
      "  n_movables: 1\n"
     ]
    }
   ],
   "source": [
    "# Create the level generator\n",
    "sample_random_level = make_level_generator(\n",
    "    height=config[\"grid_size\"],\n",
    "    width=config[\"grid_size\"],\n",
    "    n_walls=config[\"n_walls\"],\n",
    "    n_movables=config[\"n_movables\"],\n",
    ")\n",
    "\n",
    "# Generate some random levels\n",
    "rng = jax.random.PRNGKey(42)\n",
    "rngs = jax.random.split(rng, 4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    random_level = sample_random_level(rngs[i])\n",
    "    obs, state = env.reset_env_to_level(rngs[i], random_level, env_params)\n",
    "    img = renderer.render_state(state, env_params)\n",
    "    ax.imshow(np.array(img))\n",
    "    ax.set_title(f\"Random Level {i+1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Randomly Generated Levels (Domain Randomization)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLevel generator config:\")\n",
    "print(f\"  n_walls: {config['n_walls']}\")\n",
    "print(f\"  n_movables: {config['n_movables']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76def059",
   "metadata": {},
   "source": [
    "### 3.4 Taking Steps in the Environment\n",
    "\n",
    "Let's manually step through the environment to understand the dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a2e427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Trace:\n",
      "========================================\n",
      "Step 0: Initial state\n",
      "Step 1: Action=right, Reward=9.99, Done=True\n",
      "Step 2: Action=right, Reward=-0.01, Done=False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGEAAAGNCAYAAACxLmxsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQMJJREFUeJzt3Qm4VVXdOODFKIPKBVJxJFHLGcsB/ybirGWaWs6Vllk5lVr6KaZQlplaZplTlkNZ4ZRpA19lalYMTh+pTSaGnymoKKggk5z/81t17ne43OFcuKxL3Pd9nsO+nLPPWXtYe6+9f3sN3SqVSiUBAAAAsEJ1X7E/DwAAAEAQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQB6ES77757frXXfffdl7p165an7TV27Nj83c721re+Nb33ve/t7MWgjnx22223pa5u8uTJqXfv3mnatGkrPK1//OMfebvfcMMNje+dffbZacSIESs87a7kpJNOSvvss0+r2315zpfV77700kvpP0XT9V24cGHacMMN05VXXtmpywWwKhGEAVgOcbFaz2tZgiWlAyK1y7v22munkSNHph//+MfpP9XPf/7zvC7rrbdeWrx4cVrZ/OAHP0hf//rXO3sx/mPEjXE1f/7ud79b6vNKpZJvFuPzpsG9cePGpQ9+8INps802y58vS+Dz3HPPTUcddVQaOnRo6gynnXZamjJlSrrrrrs6Jf3nnnsu36D/z//8zwr5/RdeeCEHmrbZZpu0+uqrpz59+qRNN900feQjH1lqf7/++utpzJgxaf/990+DBg1aKnBSj6effjpdd911afTo0WllcOGFF6Y777wzrWx69eqVzjjjjPSlL30pzZs3r7MXB2CVIAgDsBy+973vLfGqPlVt+v4WW2zR7Pd/+ctf5ld77bbbbumNN97I046y3XbbNS7vZz/72XzTdeihh6arr746/Se6+eabc3Dp+eefT7/5zW/SykYQZtnEzXlsu6buv//+9Oyzz6bVVlttqc+uuuqq9JOf/CQHaQYOHNjuNCPw8Otf/zp98pOfTJ1lyJAh6X3ve1+69NJLOyX9OB98/vOfXyFBmKhltNVWW+XjYfvtt09f+cpX0hVXXJGOOOKI/FkEhH/72982zh81S77whS+kP//5z2n48OHLlObll1+eNt5447THHnu0Ot/nPve5fK7tqkGYEIGw2ObNHXcAtF/PZfgOAP8WT9drTZw4Mf3qV79a6v2m5s6dm/r165ebNyyL7t2755vRjrT++usvsdwf/vCH85Poyy67rFNvPpfFnDlz8k33l7/85XT99dfngMzee+/d2YtFB3jPe96Tbr311vSNb3wj9ez5f5cxcYMYN/DNNf2IwGLk7zhutt5663anGXloo402SjvvvHOr80VtnKgt0Ldv37QiHH744emwww5LU6dOTcOGDUurgldeeSUdfPDBeV9GgGfzzTdf4vMvfvGL6Uc/+tES23TdddfNwdUITD300ENpxx13bFea0cQmzgn1nNdiuWrz2coq8l2UJ5HHO1pDQ0Pad999c22jj370ox3++wBdjZowACtYNH2IG7+HH34411yJ4Eu1CnxtnzAzZszIF/vxtLmpv/71r7nKfTwdbqlPmAceeCDfoMXNYtQGiKf+p59++jI/xY0bnKjBE9X2W0qzpX4Upk+fnp+ebrDBBnlZ4qYpnuLHvE1FU4OddtopB5XixvKmm25aap6nnnoqv+oVzahivWN7HHnkkemOO+5otip9zPOpT30qveUtb0lrrLFGOuigg9I///nPvD7R9KJWvB83IOuss05ep3hy/93vfneJearb6JZbbsnV92P9Y7322muv9Pe//71xvtjnP/vZz3L/ItUmNlFrpzUxzymnnJJvHt/+9rfn342gQ20NgXDcccc1+1vN9W0RAcNdd90132RFE5D43eaaZ0RzrtbWp2rSpEm5iciAAQNyPh81alT6/e9/v8Q8r732Wm5aE8sY2zGavkUNskceeSTVI5oEzZw5My971YIFC3K/NUcffXSz34ljYXluTqOGwp577rnU9qv2a/Tf//3faYcddsiBgmuuuSZ/NmvWrLyekXasZwQ0o4ZH06ZxMV/ss9hmsR+OPfbY/F5zqoHECDDWI/rxiHwa6UezvJNPPnmp366en/70pz/lWiGx3yJgdfHFFy+Rr6uBjjiuq3m29piPwFjkx9gGcTxFQDeOmbZETbsIqEQtmKYBmBDpxD6vDbTE+sT5aVnFOSeCdfUEZps7btpz3qjdx7F/Yz/HNoxAfO06RuD4xhtvbNy2MX9LqueZCE5FTZ3YX7HfXn311bqPw+p2iO0ax/Qmm2zSmHebE8dozP/yyy+3uc0AaN3KH9oHWAXETeO73/3uHBCIm5O4kW8q3ouL5biBj/4OmvZp0aNHjxxUaEncBMWF/YknnpgGDx6cq/F/85vfzE004rP2iqfF//u//5t/q73e//73pyeeeCKdeuqp+UY1+nuIm+ZnnnlmiQBB3Mh/4AMfSMcff3y++YygRtx8xM1c3DxWxU1/aC6I05wIVMQNZdyoxTaPvibuvvvupbZfpBXb+0Mf+lCu5RBNWg444IClfi8CZPF5NRCy1lprpV/84hd5uePGJ262a1100UX5pj+adc2ePTvf0B5zzDH55qjav0i8H/smahqFCIK0JZYv8kLcAMaNaNxkx81W7Ov21vCI/RMBhG233TY37Yjfi/3R3M1aW+sToslX5PHYd5F/Y/6oQRLBiwgQRqAtRO2DCJjEdtxyyy3zsRE3d9G05J3vfGebyx355//9v/+XfvjDH+b0QuyLWK7Y11FDpiPFzXXk25aWLQKkEST4xCc+kU444YQcyIrjMI7l+G68H4HRP/zhD+mcc85pDDhUa85EcDLWP7ZLBD0jgBjHQnPipjpulmMfRYC1NREMiIBuBBrinBDLGc2yHnzwwfz96OujtjZK5KNofhi1bWL//Nd//VfunyW2cSxX5JHzzz8/ffzjH8/Ng8Iuu+ySpxGMicBC3NBH7bM4XqK5T6Tz6KOP5uBDS+K4jMBNpF1K7Is4lt/xjncs0/frPW9UxTaNpk+xbSLYGH3RRPAxgnLVmlof+9jH8jES2zfEfm7LBRdckGu/xHE5f/78/He9x+Fjjz2Wa7fEuSzyyqJFi/L8zZVNIX4v8mtsOx2qAyynCgAd5uSTT640PbWOGjUqv3f11VcvNX98Fq+qa665Js/72GOPLTHflltuWdlzzz0b/3/vvffm+WJaNXfu3KV+/8tf/nKlW7dulWnTpjW+N2bMmKWWcejQoZV999238uKLL+bXlClTKkceeWSe79RTT20xzfD000/n96+//vr8/1deeSX//5JLLml1W0WaMd9vf/vbxvdeeOGFymqrrVb5zGc+s9S88arHjBkzKj179qx8+9vfbnxvl112qbzvfe9bYr6HH344p3/aaact8f5xxx2X34/tVHX88cdX1l133cpLL720xLyxjQYMGNC47avbaIsttqjMnz+/cb7LL798qf16wAEH1L1OIb4fr4ceeqjxvdivffr0qRxyyCGN7x177LHN/m7T/X7ZZZfl/8f+bkm967N48eLKZpttVtlvv/3y31WxXTbeeOPKPvvs0/hebK84Ttor8lek+eCDD1auuOKKyhprrNG43Q877LDKHnvskf+OdY9t25KtttpqiWOuLb/+9a9zunfffXeLeXj8+PFLvH/BBRdU+vfvX/nb3/62xPtnn312pUePHpVnnnkm///OO+/M37/44osb51m0aFFl5MiRSxxTteI4jf3RmjiOevfuned98803G9+P7Ra/+93vfnep89NNN93U+F7s6yFDhlTe//73N74X2725ZVqwYEFl7bXXrmy99daVN954o/H9n/70p3n+888/v9VlHThwYGW77bZb6v1XX3218XwUr9dff73Z77e0XK354Ac/WBk8ePBS7zc9lzV33LTnvFH97kc/+tEl5o3jtWn6kV/i2K1H9bgcNmzYEuf99hyHBx98cD531JYNf/rTn3L+bO724Lnnnsvvf+UrX6lrGQFomeZIAAVELYN4UtyWeBocTZKitkPV448/npsKRCeVrantMyGqtkd1+3hSHffv8TS6LdFBcDwVjVd0dhm1Z+JJb/Vpbb1iOeKJbFSZjyfsrYmaENWn6iHSjpoE0edFragBU28tmKiiH09/ozZOVdRUiNoStcszfvz4xmFqa0XtnVqx/W6//fZ04IEH5r9ju1Zf++23X66B0bQpTezr2v5+quvYdL3aK2qAxBPpqqhhETUpojnMm2++2a7fqtZOiKYtbY0e1db6RF8eTz75ZG4OFDVbqtsn8mHUYoomU9U0It2oQRMdvS6rqFkQTUJ++tOf5uZNMW2pKdLyivUJLXXoGzUcIh/UimMntlF8pza/RK2U2E/VJmQxglcc71FTpSpqvDXNg7Wqv9ma6EQ4mmhFDa3aZlhRU2fNNdfMTeFqRS2s2v6gYl9HjYl68mv0yRI13eI4qu2nKmqGRPOipmk1FTXJmqsFFuee6vkoXlEzpyP36bJ00Nye80atpn3PRN6IZag2H1pWUWOq9rxf73EYeTDOGdEXT5xDqqLGU9O8XFXdXv9Jw20DrKw0RwIoINrs19MJb/QxEBfLUdU9qpqHCMjEjVpb1fWjyUQ0F4ghbJsGPyJQ0JYRI0bkTjCjmn70IxAX5K01I2gt4BSBm8985jO5antU14/q69HRb9N+HGpvAGov9tsK3rTm+9//fr6BjJuQ6g10NDuIm9K4Oa5W94/+WOIGNW6ia0XfHbVefPHF3KfDtddem1/NiZvQ1taregOzPOsVYojlpt72trfl5i+xnO3pJyOCetEsIppBRHOtyHeRx6J5WNP+U9pan7jxCy01o6nmwfheNGWK+aKvlAgoRUe7kTfa09Fs3JRHQCM64411j5vKWO4V6V+VkZbWNP9Ut8cf//jHvJyt5ZfIg9FfUtMgRAQiW1uOpn2UNBW/29zvxDkotnP186ro66fpb8a+inVoS0tphQjCNDeceK3oUyWGnG4qmj9Fk7VQHXWuxP5sS73njXqPnwiKLaumy1DvcRhNlyKI2dz5JPZjBAdb2l5t5T0A2iYIA1BAe0ZLiX4touZBPNWMYaMjIBM3yBGgaUnchMaNSnSaGE+M4+anf//+uU+K6L+grZoOIX6/tY4qW7r4bq4GRjyBj5oj0aFpPHE977zzcn8I0V9BbT8M8dS/I2+Q4iYk+rwIzd1gRF8x1SBMvarbLmoKtHRzE/2q1Oro9WqPevdT5Ml4Mn7vvffm2grxhD8CftF3RNSKql2Httanuo0uueSSnGebUw00RC2WqAkQfZ9EOvGdCNpF58nVPl7qEU/7o2ZHdAId31uWgGE9qn0itRRAa+7Yju0Rx+NZZ53V7HcicLasYjlaOxcsi87Mr3GumjJlSu6DqrafmqbHVEfv0+UNiK4M27dp3qv3OIwgTHtVt1dH5z2ArkgQBmAlE1XEozPPapOkv/3tb7lDz9ZEJ4sxX4yuEbUKqmpHkFle1ae3TUdXafpUvSo6lozaMPGK4EjcFHz1q1/NNVVWlAiyxI1cdHTZ9MYnnshHp61RYyieTA8dOjTftMToT7UBm6aj/kRthnhaH0GMjhzmelmeKFefdNeK/R41l6q1LmI/NTe6TnP7KZ7oR4AvXl/72tfShRdemDsNjsBMe9a12oloPNWv53tR+yOac8QraoVEp7cx+lJ7gjCHHHJIPk5iWPja5nsdrTpiT3WUsHq3R9TuaGtbRB6855578ry1tWGiE92WxHJEc8G2frf6O7U1jKI2WHx/WfJxS/m1Nq0I4NWK96qftyRqycU+jKBcBOhKiH0a54qoFRKdHbdHveeN9uqIGib1HodxrogATnPnk5byXjX/Rw1JAJaPPmEAVjLxRD/a5UcNmOjfJJoQRGCmNdWAQ+2T1fg7RijpKHHzEek0HRI5RuipFc1Dmg4HHTcHEchYliew7RmiOm6sopZFNLWJ5im1rzPPPDPPE6PqhGrfB02XP0aUqhXrHP3LRL8w0T9PU9EMaFlETaV6monVmjBhwhL9z8ToVdGnS4xyUs0Dsa3jd2ubksSIPHGTW6u5oWarT8/bu5+iWVGke+mllzbbtKS6jSKQ1XSdY5SYGD65vWlG0CJG+4mRXaLW1YpsShhNp6Lvk3pFMCH2VdQCayoCZDESTYimWPF3rEdVbKOmebAqtl0cB9VRiVoSN+Bx3oigY+054Tvf+U7+jdZG8mktv1aXv1YMzR37MIaart2H0QdTjHjVVlrRH040W4zRniKgWKI2TvStFL/78MMPt/u79Z43lmX7tjQ0eUcfh3GuiPWImooRlK6K/dVcng2xrSJQFNsOgOWjJgzASiiCCNH8JS7042K5raYW8WQ3Lr5jqNJoghRPQiNo0JFV7uOJcQzxHDcbcTEe6UWHqE37Q4kbqahZETei0fFu9GcTAYAYtjaaWi2Leoaojs5e42l0tR+J5m6mo8ZFBGqiyVbcsERwJYYLjr5jqkPNVm8Ea59MxxDNUTsk+s2JJjCxXhHEiIBIdILaXECjLZF+1OA444wz8tC+EVRoK5gQw1BHfqgdojrEUMRVsY1j/aKmSMwXQbG4yY8mMLUBnOhzIwJqcZMcAbbYj/F70T/Irrvu2q51iRo10b9M1GSJocWjOV1s78iLsd0iP8ZQxNGJbvx+BMWiNkesc2y/aEIWtaTaq7W+L2rFelaDh3EjGh2VRv9HYbfddsuv1kTnx5GH6+mPJUTAL/pmiloe1SHXI82osRbDP0c+jmYdsb/f9a535T554r3IV9Esq6XgXGyr6rDWrYmaDlF7LvJFDD190EEH5RoOsX8jr9V2wluvON7jPBTBlgioRtAgjofolySak8U+j2G5oxPs6hDVMZx4W0NpDxo0KG/b2BaRJyL/xjJGjbYIMkY/Ts31q3LFFVfkoEW1g+fIXzHke7WT3NZquET+jiZJsT2b1t5pS3vOG+393VieqJEWQcnYrrF9V8RxGCJvRBPECFpHjbQIBsa5Pb7XXF9AUasy8mq1eR4Ay6GVkZMA6KAhqmNY3OY0HaK6dnjWvn375t/6/ve/v9TnzQ0XHcOL7r333pXVV1+98pa3vKVywgkn5KGm2xpytZ5hfatiqNgYtrZfv355aNlPfOITlccff3yJNGIY59gOm2++eR52NYYkHjFiROWWW26pK83mtkk9Q1THUNqxHE899VSL84wdOzbPE9slzJkzJy/roEGD8naLYVv/+te/5nkuuuiipYa+jnk33HDDSq9evfIQvnvttVfl2muvXWq/3HrrrW0OfRtD7h599NGVhoaG/Flb6xfzRPqRH2IY2hjK+x3veMdSQ4aHX/7yl3nI4Bim+O1vf3v+TtP9fs899+Rhu9dbb708X0yPOuqoJYZVbs/6hEcffbRy6KGH5uF3Y/linQ4//PCcVnXo4zPPPLMyfPjwPMR05I/4+8orr6y0Z4jq1jSXr6rr3tyrdkjhljzyyCN53gceeKDNtKpee+21yjnnnFPZdNNN8/aNYzKGSr/00kvzsM5VM2fOrHzoQx+qrLnmmvlYib9jOza3fY844ojKrrvuWqlXDEkdx2Hk13XWWady4okn5iHk6zk/NTfU+U9+8pPKlltumYeAb7p848aNy/kx9nscT8ccc0zl2WefrXtZn3/++Zw34vfj3Be/E0Mwf/jDH15iGPumw4M394r82ZZPfepTed+0d4jq9pw3qt9tOgx8NS/XLudf/vKXym677dZ43m9tuOqWjst6j8Oq+++/v7L99tvn/Bnb+uqrr252fWfNmpXnue6661pcJgDq1y3+WZ4gDgCsSqJD5Og8OPquOeaYY9LKIp6wn3zyybkGAOVFbayooRD9DXWG6IA4akdEE8W2asLQthh+O2oQRrOpak27VfG80RGi1k+MahZN4drTyTwAzdMnDABdVgzT2twNR1Trb6uJCl1LdFoczcda6oh6RYt8uc022wjAdJDosPj444/PTQ3bqyudN2LUqmgi9bnPfU4ABqCD6BMGgC4rnu5Gh5N77LFH7rsmnorHK4axjs5YoSr654jRhTrLsgQLaF1th8jt0ZXOG9E3T23nvQAsP0EYALqsGGUmOpy84IIL8mgi0flnjLYTwzQDNMd5A4DloU8YAAAAgAL0CQMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAsl+9973tp8803z8PZNjQ0pJXNfffdl7p165any/rd2267bYUsG0BXoayAfxGEAQBadOWVV+YLyxEjRjT7+V/+8pd03HHHpU022SR9+9vfTtdee22aO3duHrJ3WS5k/5P94Ac/SF//+tc7ezEAilNW1E9ZQc/OXgAAYOV18803p7e+9a1p8uTJ6e9//3vadNNNl/g8Lp4XL16cLr/88sbPXnrppfT5z38+/7377runzrbbbrulN954I/Xu3XuFX1g//vjj6bTTTluh6QCsbJQV9VNWoCYMANCsp59+Ov3hD39IX/va19Jaa62VL7KbeuGFF/K0RNXyOXPmtGv+efPm5Yv+7t27pz59+uQpAB1LWQHtI4cBAM2KC+mBAwemAw44IH3gAx9Y6sI6nnqOGTMm/x0X3lEVPaqbx98hnnDGe/GKKue11dLj9wYNGpQveHfYYYd01113LfHbN9xwQ/7e/fffn0466aS09tprpw022KDN9vg/+tGP0uc+97m0/vrrp379+qVXX321xXb+3/rWt9KwYcNS375900477ZQeeOCB/DS2uSeycYH+pS99KS9DLPNee+2Vn/ZWxXd+9rOfpWnTpjWuc2wfgFWdsuL/KCuoh+ZIAECz4kL60EMPzVWzjzrqqHTVVVelBx98MO24447582jTftNNN6Uf//jH+bPVV189bbPNNmnnnXdOJ554YjrkkEPy98O2226bp0888UR617velS98zz777NS/f/90yy23pIMPPjjdfvvt+Tu14qI6LtTPP//8up5uXnDBBXl5P/vZz6b58+e3WK08lveUU05JI0eOTKeffnr6xz/+kZchbiSau4C/6KKL8tPR+N3Zs2eniy++OB1zzDFp0qRJ+fNzzz03v//ss8+myy67LL8X2wNgVaes+D/KCupSAQBo4qGHHqrEZcKvfvWr/P/FixdXNthgg8qnP/3pJeYbM2ZMnu/FF19sfC/+jvfis6b22muvyjbbbFOZN29e43vx27vssktls802a3zv+uuvz7+x6667VhYtWtTm8t577715/mHDhlXmzp3b7GcxDfPnz68MHjy4suOOO1YWLlzYON8NN9yQ5xs1atRS391iiy3y96ouv/zy/P5jjz3W+N4BBxxQGTp0aJvLCrCqUFYs+V1lBfXQHAkAaPbJ5jrrrJP22GOP/P+oMn3EEUfkKtxvvvnmMv3myy+/nH7zm9+kww8/PL322mu5U8Z4zZw5M+23337pySefTP/85z+X+M4JJ5yQevToUXcaxx57bK4y3pqHHnoopxm/3bPn/1UKjqeV8XSzOR/5yEeWeFIaT0XD1KlT6142gFWNsmJJygrqIQgDACwhLpzjAjouqqPDxWjPHq8YenTGjBnpnnvuWabfjd+oVCrpvPPOy9XGa1/V/gKqnTdWbbzxxu1Ko575oy1+aDp6R1xkt9Q2f6ONNlri/9UL8FdeeaVdywewqlBWLE1ZQT30CQMALCGeQD7//PP54jpezT353Hfffdv9u9FhYYi28vE0szlNL3bbelLZVHvnr1dLT1jjRgGgK1JWLE1ZQT0EYQCApS6cY4SJGBGiqTvuuCN3rnj11Ve3eBEb1dGbE6NLhF69eqW99947dZahQ4c2Pm2tVqEPixYtyp0uVjuGbK+W1htgVaSsUFawbARhAIBGb7zxRr54Puyww/LQoE2tt9566Yc//GEeJjTa/TcnhvsMs2bNWuL9uFiP4TmvueaadOqpp6Z11113ic9ffPHFxiFLV6QY5nTw4MHp29/+dm6/X23rHzcUy1NlPEbviFEvAFZ1ygplBctOEAYAaBQXzNER4kEHHdTs5zGkaFz8xkVoSxfW8dRzyy23TOPGjUtve9vb0qBBg9LWW2+dX/HEdNddd83Dk0Znh/HEM/oOmDBhQh6yc8qUKSt4DVPuNHHs2LH54n7PPffMnT/GU80bbrghbbLJJsv8lHL77bfP63zGGWfkoVlj2NEDDzyww5cfoLMpK5QVLDsd8wIAjeKCuU+fPmmfffZp9vPu3bunAw44II0fPz6PGtGS6667Lq2//vrp9NNPT0cddVS67bbb8vtxwR0jTsRvxIXsySefnKurx++ef/75qZRTTjklfeMb30jPPPNM7nfggQceyDcVDQ0Nef2XxUknnZSOPvrodP311+dpXLgDrIqUFcoKll23GKd6Ob4PALBKiM4g48ntoYcemqufA0BTygqWl5owAECXM2/evKVGq7jpppvSyy+/nPsiAABlBSuCmjAAQJdz33335erv0alkdLz4yCOPpO985ztpiy22SA8//HDuCwCArk1ZwYqgY14AoMt561vfmjbccMPc1j+eaEaHkB/+8IfTRRdd5KIagExZwYqgJgwAAABAAfqEAQAAAChAEAYAAACgAEEYOk23bt3S2LFj626Pedxxx7U7jX/84x85nRtuuGEZlhAAAAA6jiAMyyWCGxHkeOihh5b7t/7whz/koMysWbM6ZNkAKFsWVF89e/ZM66+/fg6e//Of/+yQNBYuXJg+//nPp2HDhqXVVlstT7/4xS+mRYsW1fX9GTNmpI985CNp7bXXTn379k3vfOc706233trsvD/60Y/y53369ElrrbVWOv7449NLL73UIesB0JWUKB9ac9dddzWezzfaaKM0ZsyYusuNxYsXp4svvjhtvPHG+fvbbrtt+uEPf7jUfJMnT04nnXRS2n777VOvXr3yekJrjI5Ep3njjTfyibg2CBMX2HFSbmhoWGLev/71r6l7dzFDgJXZF77whXyxOm/evDRx4sR88f273/0uPf744/kCdnl88IMfzEGTj370o2mHHXbIv3/eeeelZ555Jl177bWtfvfVV19Nu+66aw7EfPrTn05DhgxJt9xySzr88MPTzTffnI4++ujGea+66qp8Mb3XXnulr33ta+nZZ59Nl19+eX7YMGnSpOVeD4CuaEWWDy35xS9+kQ4++OC0++67p29+85vpsccey8H7F154IZ/r23LuuefmUZBOOOGEtOOOO6af/OQnubyIIMuRRx7ZON/Pf/7zdN111+UgTTwg+Nvf/rZC1odVSIyOBMvq+uuvj9G1Kg8++OBy/9Yll1ySf+vpp5+udJT4rfjNWE4AypYF//Vf/5XfHzdu3HL9/uTJk/PvnHfeeUu8/5nPfKbSrVu3ypQpU1r9/sUXX5y/f8899zS+9+abb1Z23HHHypAhQyrz58/P78W0oaGhsttuu1UWL17cOO/dd9+dv/+Nb3xjudYDoKtZ0eVDa7bccsvK8OHDKwsXLmx879xzz83lxp///OdWv/vss89WevXqVTn55JMb34tyYeTIkZUNNtigsmjRosb3p0+fXpk7d27+O+Z3i01bVC2gQ0UtltVXXz1XL4zIc/wdVbk/+9nPpjfffLPFPmFieuaZZ+a/I0perbIYfbo01yfMyy+/nH9zm222yWmsueaa6d3vfneaMmVK0fUFoGUjR47M06eeemq5fueBBx7I09onj9X/VyqVNG7cuDa/H2XRnnvu2fhe1K6MmjDTp09P999/f34vnshGk9gjjjhiierk733ve3NZE82UAFh5yoeW/OlPf8qvj3/840vUvI+ajlFu3Hbbba1+P2q9RDPYmL8qyoUTTzwx15CcMGFC4/vrrLNObuYK9dIciQ4XwZb99tsvjRgxIl166aXp17/+dfrqV7+aNtlkk3zias6hhx6aq+5FO8vLLrssveUtb8nvx0Vzc6ZOnZruvPPOdNhhh+WgTVQxv+aaa9KoUaPyCXe99dZboesIQNuqgfSBAwc2vvf666/n6uhtiXb1AwYMyH/Pnz8/T5te5Pbr1y9PH3744VZ/K77f3AVy7ff32WefFtOpvvfoo4/mPgI0jwVYOcqHlsT5OkTz1Vpxj7DBBhs0ft7a9/v375+22GKLJd7faaedGj+PZq6wLARh6HBx8oyniNFWP3zyk5/MHWJ95zvfaTEIE20oY54IwkQNmqj50pqoARNBm9oL4Q996ENp8803z+lU0wagnNmzZ+cObKMciP5Top+v6EQ3apJUnXLKKenGG29s87ciqH7fffflv9/+9rfn6e9///sceG9aQ6atzh3j+/FAYNq0aWno0KEtfn+zzTbLTzojnejEt7ZfshdffDH//corr6TBgwfXuUUAWJHlQ0uef/75PF133XWX+izee+6559r8ftRwadrJbvX32vo+tEYQhhUiAi9Nqxx+73vf67Dfj5N2bc2bqD4eVcXjQvuRRx7psHQAqN/ee++9xP8joP79738/P3WsOuuss3Inu22pfTr6nve8JwdPohlq1F6JESjiIj46TYxq5tHRe2s+9rGPpauvvjo3P4ralnFhHR3z/vjHP86fV78ftTBjnrgJiKefhxxySA7QnHrqqfnJa1RNbystAMqVDy2pnqtr7xmqoiPg6LC9re+39N3a34dlIQhDh6sO6dn0ZBlPDztKVAeP0SquvPLK9PTTTy/R34wnlACd41vf+lZ629velp94fve7302//e1vl7qI3XLLLfOrveXKz372sxwgef/735/fi9+NoUO/9KUv5SB8a6K25Q9+8IP8gOBd73pXfi9GSPr617+ea2jWfj+atsbFdQR84hXipiCa1N5xxx1tpgVAufIh+olcsGDBEk1Ho6lStVlptZlpraiN01YfLvF5S9+tfg7LShCGDtejR48VnsaFF16YmxzFUKUXXHBBGjRoUG6adNppp+UADQDlRVv5avv7aFoa7eVjOM9ozlMNXsQFeD1PEHv37p3P7VVbbbVV7jg3+v2KoH5cqMdF8Omnn56rprflAx/4QDrooINyB+4RuI8msNXq7HFjUBUX79EhYwx9HX0WRA2ceO2yyy75AUNDQ8MybRuArmxFlQ/Rr2S1c/Vw7LHH5uGvq82GolnRhhtuuMT3471q3y4tie/fe++9uRPf2iZJ1WZO+p9keQjCsNJo2uayNdGj+R577JH7f6kVzZKqnfoC0LkB+S9/+cv5XH3FFVeks88+O7//6U9/epnb/Ec5EcGYqp///Oc58N60mntrF+477rhj4/+jn5jQ3Pc32mij/KqWLdF5b7UWDgArR/kQg3/U1ravBke22267PH3ooYeWCLhEXy4xulGMmtSa+P51112X/vznPy9ROyeawtb+PiwLQRhWGtEDefVit56Td0Sma91666257f6mm266wpYRgPrtvvvu+eI3mv1ETcVoVtRRbf7jaWnUiIynlUcddVTj+3Pnzs21WCIg31pQ/sknn8z9xESnkLU1YZpzzjnnpEWLFuVaNwCsPOVD9BHWnAjYx4Ad1157bfrEJz7RWFP/qquuygH9qB1ZFTVwooZLlCfVUZfe97735XN+dH0QgaIQ9x5Rbqy//vq5diQsK0EYVhrVk2h0tHjkkUfmThAPPPDAxuBMrbho/sIXvpBHr4iT4GOPPZZuvvnmNGzYsE5YcgBacuaZZ6bDDjssVw+PPlmWpc1/iP5g4glnfDc6VIw+BaZOnZr7illjjTUa55s8eXJ+ujpmzJg0duzYxvfje7EcUbsl+hKLC/Gozh4X1LUuuuii3OxpxIgRudPfO++8M/3yl79MX/ziF5eoRQPAylE+tOSSSy7JzVD33XfffG8R5/YIqERn7bVDT0cn7XFPcf3116fjjjsuvxcdBkdwKH4jOmWP83+UBzGqXtxz1Ha/ECPvVQcgiZo3IcqMEM1ZYwRXqCUIw0ojTm7Rv0tcEI8fPz5XMY8L5eaCMKNHj05z5szJHS2OGzcut+2PC/FqdUYAVg7RXj86tb300kvTCSecsMz9hkVfAnGBHB3nRl8wMepelAH1VgkfPnx4/v6MGTMaR0GKIVLXXnvtJebbZptt8gX5XXfdlfuOiU59YySluFEAYOUrH1oSD22jQ/U418cod9GvV9xDnH/++XV9P4LyUesmyp0IFG222WZ5RKfoy6ZW3K9Ezcxa1f9H0ylBGJrqVmnapgMAAACADte9438SAAAAgKYEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAooVKnCRMmVGL20q877rijMnHixMqAAQOKpz169Oic9siRI4unPWrUqJx2LEPptBsaGnLat99+e6fs88hr8lvZtLt6fqPjOHbLpt3Vj135rWzaXT2/0XEcu2XT7urHrvxWNu2unt/qoSYMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFBAt0qlUqlnxokTJ6bnn38+lbb22munnj17punTp6fFixcXTbuhoSH169cvvfzyy2nevHlF0+7Tp08aNGhQmjt3bpo1a1bRtLt3756GDBmSFi1alF544YVU2rrrrpun8ls5XT2/7bzzzsXTXVUpKxy7pSgr5LeSlBUdS1nh2C1FWSG/rYxlRd1BmEmTJnXEcgGsdEaMGNHZi7DKUFYAqyplRcdRVgBduazoWe+PRaxm//33T6XdeOONOZp1+OGHp9mzZxdN+/TTT8/rPHbs2DRhwoSiae+yyy5pzJgxafz48emyyy4rHjkdN25cjhgfd9xxqbRY5yC/lSO/0VGUFY7dUpQV8ltJyoqOpaxw7JairJDfVsayou4gTCideUK16lakXTr9BQsW5OmcOXOKpx3Vt6rLUDrtbt26NW77ztjnVfJbOfIbHcmxW45jV34rSX6jIzl2y3Hsym8lyW9t0zEvAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFNCzPTOfe+65qbSGhoY8PeOMM9KCBQuKpj18+PA8Pfzww9Nuu+1WNO0hQ4bk6XbbbVd8u/fu3TtPBw4c2Cn7vEp+K0d+oyM5dstx7MpvJclvdCTHbjmOXfmtJPmtbd0qlUqljvnSpEmT6pkN4D/OiBEjOnsRVhnKCmBVpazoOMoKoCuXFXXXhIlYzVlnnZVKO+ecc9KgQYPS2LFj05w5c4qmfdhhh6Wddtoprbfeeql///5F0451fe6559Kaa66Z1llnnaJpv/nmm2nq1KmpZ8+eaeONN06lPfnkk3m62WabFU/76aefTosWLUrDhg1LPXr0KJr2jBkz0quvvppuuOGG9MQTTxRNe+utt07HHntsmjx5crr11luLpr366qunMWPGpJkzZ6aLLroolXbxxRcXT3NVpqxQVpSirFBWlKSs6FjKCmVFKcoKZcXKWFa0qznSAw88kEo7/fTT83TChAlp9uzZRdMeOXJknsaJMk7YnWG11VYrnvbChQvzNE4WnbXeoTPSnjZtWmP1wV69ehVNe9asWXkaJ8rSx1r37v/qHmr69OnF065W1Zw3b16nnGPoeMqK8pQVZSkrlBUsP2VFecqKspQVyoqW6JgXAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAACujZnplHjRqVSuvTp0+e7rLLLmnu3LlF0x4yZEiezpkzJ5X2+uuv5+n8+fPTyy+/XDTtN998s3FaOu1anZH24sWL83TWrFmpR48eRdOOfR223nrr1L172fjoVltt1ZjnSx/n/fv3z9O+fft2yjmGjqesKEdZoawoSVlBR1JWlKOsUFaUpKxoW7dKpVKpZ8ZJkyat+KUB6AQjRozo7EVYZSgrgFWVsqLjKCuArlxW1F0TJmI1d999dypt7733Tv369Uvjx49PCxYsKJr28OHD09ChQ9Oaa66ZVlttteLRy1dffTVN6zMtTVljStG0ey/unfafuX/qPmdOGnzPPam0Fw88ME/XevbZ4mnPXHfdtLhnzzR48ODiUePXXnstzZs3L02ePDlNnz69aNoRqd5pp53StGnT0pQphfNb795p//33z0+G7umE/Hbgv/MbHUNZ0TllRTzdXWONNYo/4Zs5c2aa031OumdwJxy7Lx6YulVSWqsT8tvMvfdOi/v1U1YUpKxYtSgrul5ZEefKOGeW9uKLL6ZKqqS71+qE/DZz79Rvcb80ePz41L1wfntt+PA0b+hQZcVKWla0qznShRdemErbYYcd8snysssuS7Nnzy6a9ujRo/PJcp111kmDBg0qXmUuTpYRgLlwWNnt3rCwIQdhes2alYZ1wj6vBmGGPf548bRnDx6cFvTsmfd7r169iqY9derUfGF96623pgceeKBo2lFdL06WcaIsfZw3NDTkk2VU1eyMc4wL646nrChfVsRF9bBhw4qmvXDhwnxhPavXrOLlVDUIEzqjnJq9ww5pQb9+yoqClBWrHmVF1yor4lxZOu1qECZ0Rjm1w+wdUr8F/dLQyy5LvQrnt6mjR+cgjLJi5SwrdMwLAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABfRsz8wDBw5MpXXv/q84UUNDQ+PfpfTu3TtP33zzzbRw4cKiaUeaeRkW904DF5bd7gMWDcjTSo8eaWEn7POqhf/e/kV165YnixYtKp704sWL83T11Vcvfqz179+/Mc+XTnvAgH/ltx49enTKOYaOp6woX1bE+aN02tXzZI9Kj+LlVK1OKaf+nceUFeUoK1Y9yoquVVZUKpXiadfqjHKq+7/rOyxqaGgsN0pZ/O/8pqxYOXWrxBFRh0mTJq34pQHoBCNGjOjsRVhlKCuAVZWyouMoK4CuXFbUXRMmYjXPPfdcKm2dddZJPXv2TM8//3zj059SIkoekbxIP6JppSPWETmOKH2vXr2Kph37esGCBfnv1VZbLZU2f/78Tk87Irfd/l0rppR4OhB5fObMmWnevHlF0+7bt28aNGhQmjNnTpo1a1bRtOPYGjJkSM7vM2bMSKWtt956xdNclSkrlBWlKCuUFSUpKzqWskJZUYqyQlmxUpYVlTpNmDAhaswUf91xxx2ViRMnVgYMGFA87dGjR+e0Z86cWSkt0oy0n3rqqeJpL1iwIKf96KOPVjpDpB2vzvDII4/ktGMblBb7OtIeOXJk8bw+atSonHbk+dJpNzQ05LRvv/32TjnHxLmNjqOsKEtZoawo+VJW0FGUFWUpK5QVJV/KirbpmBcAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKKBbpVKp1DPjpEmTUp2zdrhu3bp1atpQUlc9zjoz7REjRnRK2qsiZQWU0VWPM2XFqkFZAWV01eOsspKXFWrCAAAAAJRQqdOECRMinFT8dccdd1QmTpxYGTBgQPG0R48endOeOXNmpbRIM9J+6qmniqe9YMGCnPajjz5a6QyRdrw6wyOPPJLTjm1QWuzrSHvkyJHF8/qoUaNy2pHnS6fd0NCQ07799ts75RwT5zY6jrKiLGWFsqLkS1lBR1FWlKWsUFaUfCkr2qYmDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQQLdKpVKpZ8aJEyem559/PpW29tprp549e6bp06enxYsXF027oaEh9evXL6ffvXvZeFWs66JFi3K6kX5pCxYsyNPevXtLu5DY37HfX3755TRv3ryiaffp0ycNGjQozZ07N82aNato2pHHhwwZktf/hRdeSKWtu+66aeeddy6e7qpKWaGskPaKpaxQVqwKlBXKCmmvWMqKRSt1WVF3EGbSpEkdsVwAK50RI0Z09iKsMpQVwKpKWdFxlBXAqqqesqLuUGjEavbff/9U2o033pijWYcffniaPXt20bRPP/30vM7Dhg3L0euSImo4derUNHjw4DR06NCiaUfk8I9//GOO2m699daptEceeSRP3/nOdxZP+/HHH89R62233bb4k4Jp06almTNnprFjx6YJEyYUTXuXXXZJY8aMSePHj0+XXXZZ0bTj2Bo3blx+Inbcccel0mKd6TjKCmVFKcoKZUVJyoqOpaxQVpSirFBWrIxlRbtyQ+mTVahWFYy0S6dfrULWo0eP1KtXr6JpR5rVKlWl067q1q1bp6UdOjPtOFGWTr9aNXXOnDnF83pUF6zm+dJpRz6rHuudcY6h4ykrylFWKCtKUlbQkZQV5SgrlBUlKSvapmNeAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKKBne2Y+99xzU2kNDQ15esYZZ6QFCxYUTXv48OF5OmPGjDRr1qyiac+fPz9PX3vttTR16tSiaS9evDhPFy5cWDztWp2R9qJFi/J02rRpqXv3sjHK2Nfh8MMPT7vttlvRtIcMGZKn2223XfHjvHfv3nk6cODATjnH0PGUFeUoK5QVJSkr6EjKinKUFcqKkpQVbetWqVQqdcyXJk2aVM9sAP9xRowY0dmLsMpQVgCrKmVFx1FWAF25rKi7JkzEas4666xU2jnnnJMGDRqUxo4dm+bMmVM07cMOOyzttNNO6YYbbkhPPPFE0bS33nrrdOyxx6bJkyenW2+9tWjaq6++ehozZkyaOXNmuuiii1JpF198cZ7Kb+XIb3QUZYVjtxRlhfxWkrKiYykrHLulKCvkt5WxrGhXc6QHHngglXb66afn6YQJE9Ls2bOLpj1y5Mg8jYxbet2rVdamT59ePO1qVc158+Z1yj6vkt/Kkd/oSI7dchy78ltJ8hsdybFbjmNXfitJfmubjnkBAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACggJ7tmXnUqFGptD59+uTpLrvskubOnVs07SFDhuTp1ltvnbp3Lxuv2mqrrRqXofR279+/f5727du3U/Z5lfxWjvxGR3LsluPYld9Kkt/oSI7dchy78ltJ8lvbulUqlUod86VJkybVMxvAf5wRI0Z09iKsMpQVwKpKWdFxlBVAVy4r6q4JE7Gau+++O5W29957p379+qXx48enBQsWFE17+PDhaejQoWny5Mlp+vTpRdOOyOFOO+2Upk2blqZMmVI07d69e6f9998/zZkzJ91zzz2ptAMPPDBP5bdy5Dc6irLCsVuKskJ+K0lZ0bGUFY7dUpQV8ttKWVZU6jRhwoSoMVP8dccdd1QmTpxYGTBgQPG0R48endMeOXJk8bRHjRqV045lKJ12Q0NDTvv222/vlH0eeU1+K5t2V89vdBzHbtm0u/qxK7+VTbur5zc6jmO3bNpd/diV38qm3dXzWz10zAsAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAFCMIAAAAAFCAIAwAAAFCAIAwAAABAAYIwAAAAAAUIwgAAAAAUIAgDAAAAUIAgDAAAAEABgjAAAAAABQjCAAAAABQgCAMAAABQgCAMAAAAQAGCMAAAAAAF9GzPzAMHDkylde/+rzhRQ0ND49+l9O7dO09XX3314uvev3//xmUonfaAAQPytEePHp2yz6vkt3LkNzqSY7ccx678VpL8Rkdy7Jbj2JXfSpLf2tatUqlU6pgvTZo0qZ7ZAP7jjBgxorMXYZWhrABWVcqKjqOsALpyWVF3TZiI1Tz33HOptHXWWSf17NkzPf/882nx4sVF046oZUTyZs6cmebNm1c07b59+6ZBgwalOXPmpFmzZhVNOyKHQ4YMSYsWLUozZsxIpa233np5Kr+VI7/RUZQVjt1SlBXyW0nKio6lrHDslqKskN9WyrKiUqcJEyZEjZnirzvuuKMyceLEyoABA4qnPXr06Jz2yJEji6c9atSonHYsQ+m0Gxoactq33357p+zzyGvyW9m0u3p+o+M4dsum3dWPXfmtbNpdPb/RcRy7ZdPu6seu/FY27a6e3+qhY14AAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAAKEAQBgAAAKAAQRgAAACAAgRhAAAAAAoQhAEAAAAoQBAGAAAAoIBulUqlUs+MkyZNSnXO2uG6desm7S6Wduiq6y7t8mmPGDGiU9JeFSkrpF0y7dBV113a5dNWVnQcZYW0S6Yduuq6S3vlLCvqDsIAAAAAsOw0RwIAAAAoQBAGAAAAoABBGAAAAIACBGEAAAAAChCEAQAAAChAEAYAAACgAEEYAAAAgAIEYQAAAAAKEIQBAAAASCve/weyaVygDdEN0AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total reward: 9.98\n",
      "Episode done: False\n"
     ]
    }
   ],
   "source": [
    "# Reset to a simple level\n",
    "level = Level.from_str(prefabs[\"TrivialPush\"])\n",
    "rng = jax.random.PRNGKey(0)\n",
    "obs, state = env.reset_env_to_level(rng, level, env_params)\n",
    "\n",
    "# Take a few steps\n",
    "actions_to_take = [Actions.right, Actions.right]  # Move right twice to push M1 onto G1\n",
    "frames = [renderer.render_state(state, env_params)]\n",
    "rewards = []\n",
    "dones = []\n",
    "\n",
    "print(\"Episode Trace:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Step 0: Initial state\")\n",
    "\n",
    "for i, action in enumerate(actions_to_take):\n",
    "    rng, step_rng = jax.random.split(rng)\n",
    "    obs, state, reward, done, info = env.step_env(step_rng, state, action, env_params)\n",
    "    frames.append(renderer.render_state(state, env_params))\n",
    "    rewards.append(reward)\n",
    "    dones.append(done)\n",
    "    print(f\"Step {i+1}: Action={action.name}, Reward={reward:.2f}, Done={done}\")\n",
    "\n",
    "# Visualize the trajectory\n",
    "fig, axes = plt.subplots(1, len(frames), figsize=(4 * len(frames), 4))\n",
    "for i, (ax, frame) in enumerate(zip(axes, frames)):\n",
    "    ax.imshow(np.array(frame))\n",
    "    if i == 0:\n",
    "        ax.set_title(\"Initial\")\n",
    "    else:\n",
    "        ax.set_title(f\"After {actions_to_take[i-1].name}\\nR={rewards[i-1]:.2f}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"TrivialPush: Agent pushes M1 (red) onto G1 (light red)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal reward: {sum(rewards):.2f}\")\n",
    "print(f\"Episode done: {dones[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c2c6e9",
   "metadata": {},
   "source": [
    "## 4. The Neural Network (Actor-Critic)\n",
    "\n",
    "Now let's understand the policy network. We use an Actor-Critic architecture with:\n",
    "1. **CNN** to process the 10×10×8 observation grid\n",
    "2. **LSTM** for temporal memory (important for puzzles!)\n",
    "3. **Actor head** outputs action probabilities\n",
    "4. **Critic head** outputs state value estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b54a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PushWorldActorCritic network defined!\n",
      "Action dimension: 4 actions\n"
     ]
    }
   ],
   "source": [
    "class PushWorldActorCritic(nn.Module):\n",
    "    \"\"\"Actor-Critic network for PushWorld environment.\n",
    "\n",
    "    Architecture:\n",
    "    - Input: (batch, H, W, 8) observation grid\n",
    "    - Conv1: 32 filters, 3x3, SAME padding\n",
    "    - Conv2: 64 filters, 3x3, SAME padding\n",
    "    - Flatten: 10*10*64 = 6400\n",
    "    - Dense: 256 units\n",
    "    - LSTM: 256 hidden units (for memory)\n",
    "    - Actor: 64 -> 4 (action logits)\n",
    "    - Critic: 64 -> 1 (state value)\n",
    "    \"\"\"\n",
    "\n",
    "    action_dim: int  # Number of actions (4 for PushWorld)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, hidden):\n",
    "        obs, dones = inputs\n",
    "\n",
    "        # obs.image shape: (..., 10, 10, 8)\n",
    "        img = obs.image\n",
    "\n",
    "        # CNN embedding\n",
    "        img_embed = nn.Conv(32, kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")(img)\n",
    "        img_embed = nn.relu(img_embed)\n",
    "        img_embed = nn.Conv(64, kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")(\n",
    "            img_embed\n",
    "        )\n",
    "        img_embed = nn.relu(img_embed)\n",
    "        img_embed = img_embed.reshape(*img_embed.shape[:-3], -1)  # Flatten spatial\n",
    "\n",
    "        # Dense layer\n",
    "        embedding = nn.Dense(\n",
    "            256, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(img_embed)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        # LSTM for temporal memory\n",
    "        hidden, embedding = ResetRNN(nn.OptimizedLSTMCell(features=256))(\n",
    "            (embedding, dones), initial_carry=hidden\n",
    "        )\n",
    "\n",
    "        # Actor head\n",
    "        actor = nn.Dense(64, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        actor = nn.relu(actor)\n",
    "        actor_logits = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor)\n",
    "        pi = distrax.Categorical(logits=actor_logits)\n",
    "\n",
    "        # Critic head\n",
    "        critic = nn.Dense(64, kernel_init=orthogonal(2), bias_init=constant(0.0))(\n",
    "            embedding\n",
    "        )\n",
    "        critic = nn.relu(critic)\n",
    "        value = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return hidden, pi, jnp.squeeze(value, axis=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_dims):\n",
    "        \"\"\"Initialize LSTM hidden state.\"\"\"\n",
    "        return nn.OptimizedLSTMCell(features=256).initialize_carry(\n",
    "            jax.random.PRNGKey(0), (*batch_dims, 256)\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"PushWorldActorCritic network defined!\")\n",
    "print(f\"Action dimension: {len(Actions)} actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360520c5",
   "metadata": {},
   "source": [
    "### 4.1 Initialize and Test the Network\n",
    "\n",
    "Let's create the network and see what the output shapes look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546cdc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "  obs.image: (256, 32, 10, 10, 8) (seq_len, batch, H, W, channels)\n",
      "  dones: (256, 32) (seq_len, batch)\n",
      "\n",
      "LSTM hidden state shapes:\n",
      "  c (cell state): (32, 256)\n",
      "  h (hidden state): (32, 256)\n",
      "\n",
      "Total parameters: 2,218,021\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "network = PushWorldActorCritic(action_dim=len(Actions))\n",
    "\n",
    "# Create a dummy input to initialize parameters\n",
    "# Shape: (seq_len, batch_size, H, W, channels)\n",
    "batch_size = config[\"num_train_envs\"]\n",
    "seq_len = config[\"num_steps\"]\n",
    "\n",
    "# Get a single observation\n",
    "rng = jax.random.PRNGKey(0)\n",
    "level = sample_random_level(rng)\n",
    "obs, state = env.reset_env_to_level(rng, level, env_params)\n",
    "\n",
    "# Expand to batch: (seq_len, batch_size, 10, 10, 8)\n",
    "obs_batch = jax.tree_util.tree_map(\n",
    "    lambda x: jnp.repeat(\n",
    "        jnp.repeat(x[None, ...], batch_size, axis=0)[None, ...], seq_len, axis=0\n",
    "    ),\n",
    "    obs,\n",
    ")\n",
    "dones_batch = jnp.zeros((seq_len, batch_size), dtype=jnp.bool_)\n",
    "\n",
    "print(\"Input shapes:\")\n",
    "print(f\"  obs.image: {obs_batch.image.shape} (seq_len, batch, H, W, channels)\")\n",
    "print(f\"  dones: {dones_batch.shape} (seq_len, batch)\")\n",
    "\n",
    "# Initialize hidden state\n",
    "init_hstate = PushWorldActorCritic.initialize_carry((batch_size,))\n",
    "print(\"\\nLSTM hidden state shapes:\")\n",
    "print(f\"  c (cell state): {init_hstate[0].shape}\")\n",
    "print(f\"  h (hidden state): {init_hstate[1].shape}\")\n",
    "\n",
    "# Initialize network parameters\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = network.init(init_rng, (obs_batch, dones_batch), init_hstate)\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "def count_params(params):\n",
    "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "\n",
    "print(f\"\\nTotal parameters: {count_params(params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3535e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shapes:\n",
      "  new_hstate[0] (c): (32, 256)\n",
      "  new_hstate[1] (h): (32, 256)\n",
      "  values: (256, 32) (seq_len, batch)\n",
      "\n",
      "Policy distribution (for first timestep, first env):\n",
      "  Logits: [-1.3861334 -1.3861511 -1.3866106 -1.3862823]\n",
      "  Probs: [0.25004023 0.25003582 0.24992095 0.250003  ]\n",
      "  Actions: up=25.00%, right=25.00%, down=24.99%, left=25.00%\n",
      "\n",
      "Value estimate (first timestep, first env): -0.0211\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "new_hstate, pi, values = network.apply(params, (obs_batch, dones_batch), init_hstate)\n",
    "\n",
    "print(\"Output shapes:\")\n",
    "print(f\"  new_hstate[0] (c): {new_hstate[0].shape}\")\n",
    "print(f\"  new_hstate[1] (h): {new_hstate[1].shape}\")\n",
    "print(f\"  values: {values.shape} (seq_len, batch)\")\n",
    "\n",
    "print(\"\\nPolicy distribution (for first timestep, first env):\")\n",
    "print(f\"  Logits: {pi.logits[0, 0]}\")\n",
    "print(f\"  Probs: {jax.nn.softmax(pi.logits[0, 0])}\")\n",
    "print(\n",
    "    f\"  Actions: up={jax.nn.softmax(pi.logits[0, 0])[0]:.2%}, right={jax.nn.softmax(pi.logits[0, 0])[1]:.2%}, down={jax.nn.softmax(pi.logits[0, 0])[2]:.2%}, left={jax.nn.softmax(pi.logits[0, 0])[3]:.2%}\"\n",
    ")\n",
    "\n",
    "print(f\"\\nValue estimate (first timestep, first env): {values[0, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a95581f",
   "metadata": {},
   "source": [
    "## 5. The TrainState\n",
    "\n",
    "The `TrainState` bundles everything needed for training:\n",
    "- Network parameters\n",
    "- Optimizer state\n",
    "- **Level sampler state** (PLR buffer!)\n",
    "- Logging counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51d152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainState class defined!\n",
      "Key components:\n",
      "  - params: network parameters\n",
      "  - opt_state: optimizer state\n",
      "  - sampler: PLR level buffer\n",
      "  - update counters: track DR vs replay updates\n"
     ]
    }
   ],
   "source": [
    "class UpdateState(IntEnum):\n",
    "    \"\"\"Which type of update did we just do?\"\"\"\n",
    "\n",
    "    DR = 0  # Domain Randomization (new levels)\n",
    "    REPLAY = 1  # Replay from buffer\n",
    "\n",
    "\n",
    "class TrainState(BaseTrainState):\n",
    "    \"\"\"Extended TrainState that includes PLR level sampler.\"\"\"\n",
    "\n",
    "    # The level sampler (PLR buffer) - this is the key PLR component!\n",
    "    sampler: core.FrozenDict[str, chex.ArrayTree] = struct.field(pytree_node=True)\n",
    "\n",
    "    # Track what type of update we just did (for ACCEL)\n",
    "    update_state: UpdateState = struct.field(pytree_node=True)\n",
    "\n",
    "    # Logging counters\n",
    "    num_dr_updates: int\n",
    "    num_replay_updates: int\n",
    "    num_mutation_updates: int\n",
    "\n",
    "    # Store last level batches for visualization\n",
    "    dr_last_level_batch: chex.ArrayTree = struct.field(pytree_node=True)\n",
    "    replay_last_level_batch: chex.ArrayTree = struct.field(pytree_node=True)\n",
    "    mutation_last_level_batch: chex.ArrayTree = struct.field(pytree_node=True)\n",
    "\n",
    "\n",
    "print(\"TrainState class defined!\")\n",
    "print(\"Key components:\")\n",
    "print(\"  - params: network parameters\")\n",
    "print(\"  - opt_state: optimizer state\")\n",
    "print(\"  - sampler: PLR level buffer\")\n",
    "print(\"  - update counters: track DR vs replay updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359c304",
   "metadata": {},
   "source": [
    "## 6. The Level Sampler (PLR Buffer)\n",
    "\n",
    "This is the **heart of PLR**! The level sampler:\n",
    "1. Stores levels with their scores\n",
    "2. Samples levels based on scores (high-scoring = harder levels)\n",
    "3. Handles staleness (levels not seen recently get boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df62c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level Sampler Configuration\n",
      "========================================\n",
      "Buffer capacity: 4000 levels\n",
      "Replay probability: 80%\n",
      "Minimum fill ratio: 50%\n",
      "  → Buffer needs 2000 levels before replay\n",
      "Staleness coefficient: 0.3\n",
      "Prioritization: rank\n",
      "Temperature: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Create the level sampler\n",
    "level_sampler = LevelSampler(\n",
    "    capacity=config[\"level_buffer_capacity\"],  # Max 4000 levels\n",
    "    replay_prob=config[\"replay_prob\"],  # 80% replay, 20% new levels\n",
    "    staleness_coeff=config[\"staleness_coeff\"],  # Boost old levels\n",
    "    minimum_fill_ratio=config[\n",
    "        \"minimum_fill_ratio\"\n",
    "    ],  # Wait until 50% full before replay\n",
    "    prioritization=config[\"prioritization\"],  # \"rank\" or \"topk\"\n",
    "    prioritization_params={\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"k\": config[\"topk_k\"],\n",
    "    },\n",
    "    duplicate_check=config[\"buffer_duplicate_check\"],\n",
    ")\n",
    "\n",
    "print(\"Level Sampler Configuration\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Buffer capacity: {config['level_buffer_capacity']} levels\")\n",
    "print(f\"Replay probability: {config['replay_prob']:.0%}\")\n",
    "print(f\"Minimum fill ratio: {config['minimum_fill_ratio']:.0%}\")\n",
    "print(\n",
    "    f\"  → Buffer needs {int(config['level_buffer_capacity'] * config['minimum_fill_ratio'])} levels before replay\"\n",
    ")\n",
    "print(f\"Staleness coefficient: {config['staleness_coeff']}\")\n",
    "print(f\"Prioritization: {config['prioritization']}\")\n",
    "print(f\"Temperature: {config['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67669e",
   "metadata": {},
   "source": [
    "### 6.1 Initialize the Level Sampler\n",
    "\n",
    "Let's see what the sampler state looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f0d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampler State Structure:\n",
      "========================================\n",
      "  levels: Level\n",
      "  scores: shape=(4000,), dtype=float32\n",
      "  timestamps: shape=(4000,), dtype=int32\n",
      "  size: int\n",
      "  episode_count: int\n",
      "  levels_extra: dict\n",
      "\n",
      "Current buffer size: 0\n",
      "Episode count: 0\n",
      "Scores shape: (4000,) (one score per slot)\n"
     ]
    }
   ],
   "source": [
    "# Initialize with a placeholder level\n",
    "rng = jax.random.PRNGKey(0)\n",
    "placeholder_level = sample_random_level(rng)\n",
    "\n",
    "sampler_state = level_sampler.initialize(\n",
    "    placeholder_level,\n",
    "    {\"max_return\": -jnp.inf},  # Extra data stored per level\n",
    ")\n",
    "\n",
    "print(\"Sampler State Structure:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in sampler_state.items():\n",
    "    if isinstance(value, jnp.ndarray):\n",
    "        print(f\"  {key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__}\")\n",
    "\n",
    "print()\n",
    "print(f\"Current buffer size: {sampler_state['size']}\")\n",
    "print(f\"Episode count: {sampler_state['episode_count']}\")\n",
    "print(f\"Scores shape: {sampler_state['scores'].shape} (one score per slot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41858f7",
   "metadata": {},
   "source": [
    "### 6.2 Inserting Levels into the Buffer\n",
    "\n",
    "When we generate new levels, we compute a **score** for each and insert them into the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7450fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting batch of levels:\n",
      "  Batch size: 10\n",
      "  Scores: [0.947667   0.9785799  0.33229148 0.46866846 0.5698887  0.16550303\n",
      " 0.3101946  0.68948054 0.74676657 0.17101455]\n",
      "\n",
      "After insertion:\n",
      "  Buffer size: 10\n",
      "  Inserted at indices: [0 1 2 3 4 5 6 7 8 9]\n",
      "  Scores in buffer: [0.947667   0.9785799  0.33229148 0.46866846 0.5698887  0.16550303\n",
      " 0.3101946  0.68948054 0.74676657 0.17101455]\n"
     ]
    }
   ],
   "source": [
    "# Generate a batch of levels\n",
    "rng = jax.random.PRNGKey(42)\n",
    "batch_size = 10\n",
    "rngs = jax.random.split(rng, batch_size)\n",
    "level_batch = jax.vmap(sample_random_level)(rngs)\n",
    "\n",
    "# Simulate scores (in practice, these come from the policy's performance)\n",
    "# High score = level is challenging (agent learning a lot)\n",
    "fake_scores = jax.random.uniform(jax.random.PRNGKey(0), (batch_size,))\n",
    "fake_max_returns = jnp.zeros(batch_size) + 0.5\n",
    "\n",
    "print(\"Inserting batch of levels:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Scores: {fake_scores}\")\n",
    "\n",
    "# Insert into buffer\n",
    "sampler_state, inserted_indices = level_sampler.insert_batch(\n",
    "    sampler_state, level_batch, fake_scores, {\"max_return\": fake_max_returns}\n",
    ")\n",
    "\n",
    "print(\"\\nAfter insertion:\")\n",
    "print(f\"  Buffer size: {sampler_state['size']}\")\n",
    "print(f\"  Inserted at indices: {inserted_indices}\")\n",
    "print(f\"  Scores in buffer: {sampler_state['scores'][:batch_size]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc405f32",
   "metadata": {},
   "source": [
    "## 7. GAE: Generalized Advantage Estimation\n",
    "\n",
    "Before we can do PPO updates, we need to compute **advantages**. GAE smoothly combines:\n",
    "- **TD(0)**: Low variance, high bias  \n",
    "- **Monte Carlo**: High variance, low bias\n",
    "\n",
    "The formula: `δ_t = r_t + γ * V(s_{t+1}) - V(s_t)`  \n",
    "`A_t = δ_t + (γλ) * δ_{t+1} + (γλ)² * δ_{t+2} + ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05c2443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_gae function defined!\n",
      "Hyperparameters:\n",
      "  gamma (discount): 0.995\n",
      "  lambda (GAE): 0.98\n"
     ]
    }
   ],
   "source": [
    "def compute_gae(\n",
    "    gamma: float,\n",
    "    lambd: float,\n",
    "    last_value: chex.Array,\n",
    "    values: chex.Array,\n",
    "    rewards: chex.Array,\n",
    "    dones: chex.Array,\n",
    ") -> Tuple[chex.Array, chex.Array]:\n",
    "    \"\"\"Compute Generalized Advantage Estimation.\n",
    "\n",
    "    Args:\n",
    "        gamma: Discount factor (default 0.995)\n",
    "        lambd: GAE lambda (default 0.98)\n",
    "        last_value: Value of last state, shape (NUM_ENVS,)\n",
    "        values: Values for each step, shape (NUM_STEPS, NUM_ENVS)\n",
    "        rewards: Rewards for each step, shape (NUM_STEPS, NUM_ENVS)\n",
    "        dones: Done flags for each step, shape (NUM_STEPS, NUM_ENVS)\n",
    "\n",
    "    Returns:\n",
    "        advantages: shape (NUM_STEPS, NUM_ENVS)\n",
    "        targets: shape (NUM_STEPS, NUM_ENVS) = advantages + values\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_gae_at_timestep(carry, x):\n",
    "        gae, next_value = carry\n",
    "        value, reward, done = x\n",
    "\n",
    "        # TD error\n",
    "        delta = reward + gamma * next_value * (1 - done) - value\n",
    "        # GAE accumulation\n",
    "        gae = delta + gamma * lambd * (1 - done) * gae\n",
    "\n",
    "        return (gae, value), gae\n",
    "\n",
    "    # Scan backwards through time\n",
    "    _, advantages = jax.lax.scan(\n",
    "        compute_gae_at_timestep,\n",
    "        (jnp.zeros_like(last_value), last_value),\n",
    "        (values, rewards, dones),\n",
    "        reverse=True,  # Important! We scan backwards\n",
    "        unroll=16,\n",
    "    )\n",
    "\n",
    "    # Targets for value function = advantages + values\n",
    "    return advantages, advantages + values\n",
    "\n",
    "\n",
    "print(\"compute_gae function defined!\")\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"  gamma (discount): {config['gamma']}\")\n",
    "print(f\"  lambda (GAE): {config['gae_lambda']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937098f",
   "metadata": {},
   "source": [
    "## 8. Trajectory Sampling (Rollouts)\n",
    "\n",
    "We need to collect trajectories from the environment to train on. This involves:\n",
    "1. Reset environments to levels\n",
    "2. Run policy for `num_steps` steps\n",
    "3. Collect (obs, action, reward, done, log_prob, value) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0cffa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_trajectories_rnn function defined!\n",
      "\n",
      "Rollout configuration:\n",
      "  num_envs: 32\n",
      "  num_steps: 256\n",
      "  Total transitions per rollout: 8,192\n"
     ]
    }
   ],
   "source": [
    "def sample_trajectories_rnn(\n",
    "    rng: chex.PRNGKey,\n",
    "    env,\n",
    "    env_params: EnvParams,\n",
    "    train_state: TrainState,\n",
    "    init_hstate: chex.ArrayTree,\n",
    "    init_obs: Observation,\n",
    "    init_env_state: EnvState,\n",
    "    num_envs: int,\n",
    "    max_episode_length: int,\n",
    "):\n",
    "    \"\"\"Sample trajectories using the RNN policy.\n",
    "\n",
    "    This is the main rollout function. It runs the policy for max_episode_length\n",
    "    steps across num_envs parallel environments.\n",
    "\n",
    "    Returns:\n",
    "        final_carry: (rng, train_state, hstate, last_obs, last_env_state, last_value)\n",
    "        trajectories: (obs, actions, rewards, dones, log_probs, values, info)\n",
    "    \"\"\"\n",
    "\n",
    "    def sample_step(carry, _):\n",
    "        rng, train_state, hstate, obs, env_state, last_done = carry\n",
    "        rng, rng_action, rng_step = jax.random.split(rng, 3)\n",
    "\n",
    "        # Add sequence dimension for network\n",
    "        x = jax.tree_util.tree_map(lambda x: x[None, ...], (obs, last_done))\n",
    "\n",
    "        # Forward pass through network\n",
    "        hstate, pi, value = train_state.apply_fn(train_state.params, x, hstate)\n",
    "\n",
    "        # Sample action from policy\n",
    "        action = pi.sample(seed=rng_action)\n",
    "        log_prob = pi.log_prob(action)\n",
    "\n",
    "        # Remove sequence dimension\n",
    "        value, action, log_prob = (\n",
    "            value.squeeze(0),\n",
    "            action.squeeze(0),\n",
    "            log_prob.squeeze(0),\n",
    "        )\n",
    "\n",
    "        # Step all environments in parallel\n",
    "        next_obs, env_state, reward, done, info = jax.vmap(\n",
    "            env.step, in_axes=(0, 0, 0, None)\n",
    "        )(jax.random.split(rng_step, num_envs), env_state, action, env_params)\n",
    "\n",
    "        carry = (rng, train_state, hstate, next_obs, env_state, done)\n",
    "        return carry, (obs, action, reward, done, log_prob, value, info)\n",
    "\n",
    "    # Run the rollout\n",
    "    (rng, train_state, hstate, last_obs, last_env_state, last_done), traj = (\n",
    "        jax.lax.scan(\n",
    "            sample_step,\n",
    "            (\n",
    "                rng,\n",
    "                train_state,\n",
    "                init_hstate,\n",
    "                init_obs,\n",
    "                init_env_state,\n",
    "                jnp.zeros(num_envs, dtype=bool),\n",
    "            ),\n",
    "            None,\n",
    "            length=max_episode_length,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Get value of final state (for GAE computation)\n",
    "    x = jax.tree_util.tree_map(lambda x: x[None, ...], (last_obs, last_done))\n",
    "    _, _, last_value = train_state.apply_fn(train_state.params, x, hstate)\n",
    "\n",
    "    return (\n",
    "        rng,\n",
    "        train_state,\n",
    "        hstate,\n",
    "        last_obs,\n",
    "        last_env_state,\n",
    "        last_value.squeeze(0),\n",
    "    ), traj\n",
    "\n",
    "\n",
    "print(\"sample_trajectories_rnn function defined!\")\n",
    "print(\"\\nRollout configuration:\")\n",
    "print(f\"  num_envs: {config['num_train_envs']}\")\n",
    "print(f\"  num_steps: {config['num_steps']}\")\n",
    "print(\n",
    "    f\"  Total transitions per rollout: {config['num_train_envs'] * config['num_steps']:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66029693",
   "metadata": {},
   "source": [
    "## 9. PPO Update\n",
    "\n",
    "PPO (Proximal Policy Optimization) is our policy gradient algorithm. Key ideas:\n",
    "1. **Clipped surrogate objective**: Prevents too-large policy updates\n",
    "2. **Value function loss**: Train critic to predict returns\n",
    "3. **Entropy bonus**: Encourage exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e38b4dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_actor_critic_rnn function defined!\n",
      "\n",
      "PPO configuration:\n",
      "  clip_eps: 0.2\n",
      "  n_epochs: 5\n",
      "  n_minibatches: 1\n",
      "  entropy_coeff: 0.001\n",
      "  critic_coeff: 0.5\n"
     ]
    }
   ],
   "source": [
    "def update_actor_critic_rnn(\n",
    "    rng: chex.PRNGKey,\n",
    "    train_state: TrainState,\n",
    "    init_hstate: chex.ArrayTree,\n",
    "    batch: chex.ArrayTree,\n",
    "    num_envs: int,\n",
    "    n_steps: int,\n",
    "    n_minibatch: int,\n",
    "    n_epochs: int,\n",
    "    clip_eps: float,\n",
    "    entropy_coeff: float,\n",
    "    critic_coeff: float,\n",
    "    update_grad: bool = True,\n",
    "):\n",
    "    \"\"\"Update the actor-critic network using PPO.\n",
    "\n",
    "    Args:\n",
    "        batch: (obs, actions, dones, log_probs, values, targets, advantages)\n",
    "        update_grad: If False, compute losses but don't update params (for DR in robust PLR)\n",
    "    \"\"\"\n",
    "    obs, actions, dones, log_probs, values, targets, advantages = batch\n",
    "\n",
    "    # Shift dones to get \"last_done\" (done from previous step)\n",
    "    last_dones = jnp.roll(dones, 1, axis=0).at[0].set(False)\n",
    "    batch = obs, actions, last_dones, log_probs, values, targets, advantages\n",
    "\n",
    "    def update_epoch(carry, _):\n",
    "        \"\"\"One epoch of PPO updates over all minibatches.\"\"\"\n",
    "\n",
    "        def update_minibatch(train_state, minibatch):\n",
    "            \"\"\"Update on a single minibatch.\"\"\"\n",
    "            (\n",
    "                init_hstate,\n",
    "                obs,\n",
    "                actions,\n",
    "                last_dones,\n",
    "                log_probs,\n",
    "                values,\n",
    "                targets,\n",
    "                advantages,\n",
    "            ) = minibatch\n",
    "\n",
    "            def loss_fn(params):\n",
    "                # Forward pass\n",
    "                _, pi, values_pred = train_state.apply_fn(\n",
    "                    params, (obs, last_dones), init_hstate\n",
    "                )\n",
    "                log_probs_pred = pi.log_prob(actions)\n",
    "                entropy = pi.entropy().mean()\n",
    "\n",
    "                # PPO clipped objective\n",
    "                ratio = jnp.exp(log_probs_pred - log_probs)\n",
    "                A = (advantages - advantages.mean()) / (\n",
    "                    advantages.std() + 1e-5\n",
    "                )  # Normalize\n",
    "                l_clip = -jnp.minimum(\n",
    "                    ratio * A, jnp.clip(ratio, 1 - clip_eps, 1 + clip_eps) * A\n",
    "                ).mean()\n",
    "\n",
    "                # Clipped value loss\n",
    "                values_pred_clipped = values + (values_pred - values).clip(\n",
    "                    -clip_eps, clip_eps\n",
    "                )\n",
    "                l_vf = (\n",
    "                    0.5\n",
    "                    * jnp.maximum(\n",
    "                        (values_pred - targets) ** 2,\n",
    "                        (values_pred_clipped - targets) ** 2,\n",
    "                    ).mean()\n",
    "                )\n",
    "\n",
    "                # Total loss\n",
    "                loss = l_clip + critic_coeff * l_vf - entropy_coeff * entropy\n",
    "\n",
    "                return loss, (l_vf, l_clip, entropy)\n",
    "\n",
    "            grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "            loss, grads = grad_fn(train_state.params)\n",
    "            if update_grad:\n",
    "                train_state = train_state.apply_gradients(grads=grads)\n",
    "            return train_state, loss\n",
    "\n",
    "        rng, train_state = carry\n",
    "        rng, rng_perm = jax.random.split(rng)\n",
    "\n",
    "        # Shuffle and create minibatches\n",
    "        permutation = jax.random.permutation(rng_perm, num_envs)\n",
    "        minibatches = (\n",
    "            jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=0).reshape(\n",
    "                    n_minibatch, -1, *x.shape[1:]\n",
    "                ),\n",
    "                init_hstate,\n",
    "            ),\n",
    "            *jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=1)\n",
    "                .reshape(x.shape[0], n_minibatch, -1, *x.shape[2:])\n",
    "                .swapaxes(0, 1),\n",
    "                batch,\n",
    "            ),\n",
    "        )\n",
    "        train_state, losses = jax.lax.scan(update_minibatch, train_state, minibatches)\n",
    "        return (rng, train_state), losses\n",
    "\n",
    "    return jax.lax.scan(update_epoch, (rng, train_state), None, n_epochs)\n",
    "\n",
    "\n",
    "print(\"update_actor_critic_rnn function defined!\")\n",
    "print(\"\\nPPO configuration:\")\n",
    "print(f\"  clip_eps: {config['clip_eps']}\")\n",
    "print(f\"  n_epochs: {config['epoch_ppo']}\")\n",
    "print(f\"  n_minibatches: {config['num_minibatches']}\")\n",
    "print(f\"  entropy_coeff: {config['entropy_coeff']}\")\n",
    "print(f\"  critic_coeff: {config['critic_coeff']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146efeb9",
   "metadata": {},
   "source": [
    "## 10. Level Scoring (The PLR Magic)\n",
    "\n",
    "This is where PLR gets interesting! We need to score each level to determine how useful it is for training.\n",
    "\n",
    "**MaxMC** (Maximum Monte-Carlo): max(V - V_max) over an episode\n",
    "- High score means agent thinks level is valuable but actual returns were bad\n",
    "- Indicates learning opportunity!\n",
    "\n",
    "**Positive Value Loss (pvl)**: mean(max(0, A)) over episode\n",
    "- Measures positive advantages (when actions were better than expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b625abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score function: MaxMC\n",
      "\n",
      "Intuition:\n",
      "  - High score = agent is learning from this level\n",
      "  - Low score = agent already mastered it (or it's impossible)\n",
      "  - PLR prioritizes high-scoring levels for replay\n"
     ]
    }
   ],
   "source": [
    "def compute_score(config, dones, values, max_returns, advantages):\n",
    "    \"\"\"Compute PLR score for levels.\n",
    "\n",
    "    Args:\n",
    "        dones: (num_steps, num_envs) - done flags\n",
    "        values: (num_steps, num_envs) - value estimates\n",
    "        max_returns: (num_envs,) - maximum return achieved\n",
    "        advantages: (num_steps, num_envs) - GAE advantages\n",
    "\n",
    "    Returns:\n",
    "        scores: (num_envs,) - one score per level\n",
    "    \"\"\"\n",
    "    if config[\"score_function\"] == \"MaxMC\":\n",
    "        # max_mc computes: max_t (V_t - max_return)\n",
    "        # High when agent overestimates value vs actual performance\n",
    "        return max_mc(dones, values, max_returns)\n",
    "    elif config[\"score_function\"] == \"pvl\":\n",
    "        # positive_value_loss: mean of positive advantages\n",
    "        return positive_value_loss(dones, advantages)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown score function: {config['score_function']}\")\n",
    "\n",
    "\n",
    "print(f\"Score function: {config['score_function']}\")\n",
    "print()\n",
    "print(\"Intuition:\")\n",
    "print(\"  - High score = agent is learning from this level\")\n",
    "print(\"  - Low score = agent already mastered it (or it's impossible)\")\n",
    "print(\"  - PLR prioritizes high-scoring levels for replay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e9e80",
   "metadata": {},
   "source": [
    "## 11. Create the TrainState\n",
    "\n",
    "Now let's put it all together and create the initial training state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e642b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap environment for auto-replay (resets to same level when done)\n",
    "wrapped_env = AutoReplayWrapper(env)\n",
    "\n",
    "\n",
    "def create_train_state(rng) -> TrainState:\n",
    "    \"\"\"Initialize all components for training.\"\"\"\n",
    "\n",
    "    # Learning rate schedule: linear decay\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"num_minibatches\"] * config[\"epoch_ppo\"]))\n",
    "            / config[\"num_updates\"]\n",
    "        )\n",
    "        return config[\"lr\"] * frac\n",
    "\n",
    "    # Create dummy observation for network initialization\n",
    "    rng, rng_level, rng_reset = jax.random.split(rng, 3)\n",
    "    dummy_level = sample_random_level(rng_level)\n",
    "    obs, _ = wrapped_env.reset_to_level(rng_reset, dummy_level, env_params)\n",
    "\n",
    "    # Expand to batch shape\n",
    "    obs = jax.tree_util.tree_map(\n",
    "        lambda x: jnp.repeat(\n",
    "            jnp.repeat(x[None, ...], config[\"num_train_envs\"], axis=0)[None, ...],\n",
    "            config[\"num_steps\"],\n",
    "            axis=0,\n",
    "        ),\n",
    "        obs,\n",
    "    )\n",
    "    init_x = (obs, jnp.zeros((config[\"num_steps\"], config[\"num_train_envs\"])))\n",
    "\n",
    "    # Initialize network\n",
    "    network = PushWorldActorCritic(len(Actions))\n",
    "    rng, rng_init = jax.random.split(rng)\n",
    "    network_params = network.init(\n",
    "        rng_init,\n",
    "        init_x,\n",
    "        PushWorldActorCritic.initialize_carry((config[\"num_train_envs\"],)),\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(config[\"max_grad_norm\"]),\n",
    "        optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "    )\n",
    "\n",
    "    # Initialize level sampler\n",
    "    placeholder_level = sample_random_level(jax.random.PRNGKey(0))\n",
    "    sampler = level_sampler.initialize(placeholder_level, {\"max_return\": -jnp.inf})\n",
    "\n",
    "    # Placeholder level batch\n",
    "    placeholder_level_batch = jax.tree_util.tree_map(\n",
    "        lambda x: jnp.array([x]).repeat(config[\"num_train_envs\"], axis=0),\n",
    "        placeholder_level,\n",
    "    )\n",
    "\n",
    "    return TrainState.create(\n",
    "        apply_fn=network.apply,\n",
    "        params=network_params,\n",
    "        tx=tx,\n",
    "        sampler=sampler,\n",
    "        update_state=0,\n",
    "        num_dr_updates=0,\n",
    "        num_replay_updates=0,\n",
    "        num_mutation_updates=0,\n",
    "        dr_last_level_batch=placeholder_level_batch,\n",
    "        replay_last_level_batch=placeholder_level_batch,\n",
    "        mutation_last_level_batch=placeholder_level_batch,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create the initial train state\n",
    "rng = jax.random.PRNGKey(config[\"seed\"])\n",
    "rng, rng_init = jax.random.split(rng)\n",
    "train_state = create_train_state(rng_init)\n",
    "\n",
    "print(\"TrainState created!\")\n",
    "print(f\"  Network params: {count_params(train_state.params):,}\")\n",
    "print(f\"  Sampler buffer size: {train_state.sampler['size']}\")\n",
    "print(f\"  DR updates: {train_state.num_dr_updates}\")\n",
    "print(f\"  Replay updates: {train_state.num_replay_updates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2d8dd",
   "metadata": {},
   "source": [
    "## 12. The PLR Training Loop\n",
    "\n",
    "The PLR training loop has three branches:\n",
    "1. **on_new_levels** (Domain Randomization): Generate new random levels, score them, add to buffer\n",
    "2. **on_replay_levels**: Sample levels from buffer, train on them, update scores\n",
    "3. **on_mutate_levels** (ACCEL only): Mutate replay levels to create variations\n",
    "\n",
    "The choice between branches depends on:\n",
    "- Buffer fill ratio (need minimum fill before replay)\n",
    "- Replay probability (80% replay vs 20% new levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bc275",
   "metadata": {},
   "source": [
    "### 12.1 on_new_levels: Domain Randomization\n",
    "\n",
    "This function generates new random levels and adds them to the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274facfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_new_levels(rng: chex.PRNGKey, train_state: TrainState):\n",
    "    \"\"\"Generate new random levels, evaluate policy, score and add to buffer.\n",
    "\n",
    "    This is the 'Domain Randomization' branch of PLR.\n",
    "    In Robust PLR, we DON'T update gradients on these levels.\n",
    "    \"\"\"\n",
    "    sampler = train_state.sampler\n",
    "\n",
    "    # Step 1: Generate new random levels\n",
    "    rng, rng_levels, rng_reset = jax.random.split(rng, 3)\n",
    "    new_levels = jax.vmap(sample_random_level)(\n",
    "        jax.random.split(rng_levels, config[\"num_train_envs\"])\n",
    "    )\n",
    "\n",
    "    # Step 2: Reset environments to these levels\n",
    "    init_obs, init_env_state = jax.vmap(\n",
    "        wrapped_env.reset_to_level, in_axes=(0, 0, None)\n",
    "    )(\n",
    "        jax.random.split(rng_reset, config[\"num_train_envs\"]),\n",
    "        new_levels,\n",
    "        env_params,\n",
    "    )\n",
    "\n",
    "    # Step 3: Collect trajectories\n",
    "    (\n",
    "        (rng, train_state, hstate, last_obs, last_env_state, last_value),\n",
    "        (obs, actions, rewards, dones, log_probs, values, info),\n",
    "    ) = sample_trajectories_rnn(\n",
    "        rng,\n",
    "        wrapped_env,\n",
    "        env_params,\n",
    "        train_state,\n",
    "        PushWorldActorCritic.initialize_carry((config[\"num_train_envs\"],)),\n",
    "        init_obs,\n",
    "        init_env_state,\n",
    "        config[\"num_train_envs\"],\n",
    "        config[\"num_steps\"],\n",
    "    )\n",
    "\n",
    "    # Step 4: Compute advantages and scores\n",
    "    advantages, targets = compute_gae(\n",
    "        config[\"gamma\"], config[\"gae_lambda\"], last_value, values, rewards, dones\n",
    "    )\n",
    "    max_returns = compute_max_returns(dones, rewards)\n",
    "    scores = compute_score(config, dones, values, max_returns, advantages)\n",
    "\n",
    "    # Step 5: Insert levels into buffer\n",
    "    sampler, _ = level_sampler.insert_batch(\n",
    "        sampler, new_levels, scores, {\"max_return\": max_returns}\n",
    "    )\n",
    "\n",
    "    # Step 6: Update policy (only if exploratory_grad_updates is True)\n",
    "    (rng, train_state), losses = update_actor_critic_rnn(\n",
    "        rng,\n",
    "        train_state,\n",
    "        PushWorldActorCritic.initialize_carry((config[\"num_train_envs\"],)),\n",
    "        (obs, actions, dones, log_probs, values, targets, advantages),\n",
    "        config[\"num_train_envs\"],\n",
    "        config[\"num_steps\"],\n",
    "        config[\"num_minibatches\"],\n",
    "        config[\"epoch_ppo\"],\n",
    "        config[\"clip_eps\"],\n",
    "        config[\"entropy_coeff\"],\n",
    "        config[\"critic_coeff\"],\n",
    "        update_grad=config[\"exploratory_grad_updates\"],  # False for Robust PLR!\n",
    "    )\n",
    "\n",
    "    # Update train state\n",
    "    train_state = train_state.replace(\n",
    "        sampler=sampler,\n",
    "        update_state=UpdateState.DR,\n",
    "        num_dr_updates=train_state.num_dr_updates + 1,\n",
    "        dr_last_level_batch=new_levels,\n",
    "    )\n",
    "\n",
    "    return (rng, train_state), {\"scores\": scores, \"rewards\": rewards}\n",
    "\n",
    "\n",
    "print(\"on_new_levels function defined!\")\n",
    "print(f\"  exploratory_grad_updates: {config['exploratory_grad_updates']}\")\n",
    "print(\n",
    "    \"  → Gradients will {'be' if config['exploratory_grad_updates'] else 'NOT be'} applied on DR levels\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3767a48b",
   "metadata": {},
   "source": [
    "### 12.2 on_replay_levels: Replay from Buffer\n",
    "\n",
    "This function samples levels from the buffer and trains on them. This is where the actual learning happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eac4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_replay_levels(rng: chex.PRNGKey, train_state: TrainState):\n",
    "    \"\"\"Sample levels from buffer, train policy, update scores.\n",
    "\n",
    "    This is where the actual learning happens in PLR!\n",
    "    \"\"\"\n",
    "    sampler = train_state.sampler\n",
    "\n",
    "    # Step 1: Sample levels from buffer (weighted by score)\n",
    "    rng, rng_levels, rng_reset = jax.random.split(rng, 3)\n",
    "    sampler, (level_inds, levels) = level_sampler.sample_replay_levels(\n",
    "        sampler, rng_levels, config[\"num_train_envs\"]\n",
    "    )\n",
    "\n",
    "    # Step 2: Reset environments to sampled levels\n",
    "    init_obs, init_env_state = jax.vmap(\n",
    "        wrapped_env.reset_to_level, in_axes=(0, 0, None)\n",
    "    )(\n",
    "        jax.random.split(rng_reset, config[\"num_train_envs\"]),\n",
    "        levels,\n",
    "        env_params,\n",
    "    )\n",
    "\n",
    "    # Step 3: Collect trajectories\n",
    "    (\n",
    "        (rng, train_state, hstate, last_obs, last_env_state, last_value),\n",
    "        (obs, actions, rewards, dones, log_probs, values, info),\n",
    "    ) = sample_trajectories_rnn(\n",
    "        rng,\n",
    "        wrapped_env,\n",
    "        env_params,\n",
    "        train_state,\n",
    "        PushWorldActorCritic.initialize_carry((config[\"num_train_envs\"],)),\n",
    "        init_obs,\n",
    "        init_env_state,\n",
    "        config[\"num_train_envs\"],\n",
    "        config[\"num_steps\"],\n",
    "    )\n",
    "\n",
    "    # Step 4: Compute advantages and update scores\n",
    "    advantages, targets = compute_gae(\n",
    "        config[\"gamma\"], config[\"gae_lambda\"], last_value, values, rewards, dones\n",
    "    )\n",
    "\n",
    "    # Keep track of best return seen on each level\n",
    "    max_returns = jnp.maximum(\n",
    "        level_sampler.get_levels_extra(sampler, level_inds)[\"max_return\"],\n",
    "        compute_max_returns(dones, rewards),\n",
    "    )\n",
    "    scores = compute_score(config, dones, values, max_returns, advantages)\n",
    "\n",
    "    # Update scores in buffer\n",
    "    sampler = level_sampler.update_batch(\n",
    "        sampler, level_inds, scores, {\"max_return\": max_returns}\n",
    "    )\n",
    "\n",
    "    # Step 5: Update policy (ALWAYS apply gradients on replay!)\n",
    "    (rng, train_state), losses = update_actor_critic_rnn(\n",
    "        rng,\n",
    "        train_state,\n",
    "        PushWorldActorCritic.initialize_carry((config[\"num_train_envs\"],)),\n",
    "        (obs, actions, dones, log_probs, values, targets, advantages),\n",
    "        config[\"num_train_envs\"],\n",
    "        config[\"num_steps\"],\n",
    "        config[\"num_minibatches\"],\n",
    "        config[\"epoch_ppo\"],\n",
    "        config[\"clip_eps\"],\n",
    "        config[\"entropy_coeff\"],\n",
    "        config[\"critic_coeff\"],\n",
    "        update_grad=True,  # Always update on replay levels!\n",
    "    )\n",
    "\n",
    "    # Update train state\n",
    "    train_state = train_state.replace(\n",
    "        sampler=sampler,\n",
    "        update_state=UpdateState.REPLAY,\n",
    "        num_replay_updates=train_state.num_replay_updates + 1,\n",
    "        replay_last_level_batch=levels,\n",
    "    )\n",
    "\n",
    "    return (rng, train_state), {\n",
    "        \"scores\": scores,\n",
    "        \"rewards\": rewards,\n",
    "        \"level_inds\": level_inds,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"on_replay_levels function defined!\")\n",
    "print(\"  → Gradients are ALWAYS applied on replay levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b815a",
   "metadata": {},
   "source": [
    "### 12.3 The Main Train Step\n",
    "\n",
    "The train step decides which branch to take based on buffer state and randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(carry, _):\n",
    "    \"\"\"One step of PLR training.\n",
    "\n",
    "    Decides between:\n",
    "    - on_new_levels (DR): Generate new levels, add to buffer\n",
    "    - on_replay_levels: Sample from buffer, train policy\n",
    "    \"\"\"\n",
    "    rng, train_state = carry\n",
    "    rng, rng_replay = jax.random.split(rng)\n",
    "\n",
    "    # Decide: replay or generate new levels?\n",
    "    # sample_replay_decision returns True if we should replay\n",
    "    branch = level_sampler.sample_replay_decision(\n",
    "        train_state.sampler, rng_replay\n",
    "    ).astype(int)\n",
    "\n",
    "    # branch = 0 -> on_new_levels (DR)\n",
    "    # branch = 1 -> on_replay_levels\n",
    "    return jax.lax.switch(\n",
    "        branch,\n",
    "        [on_new_levels, on_replay_levels],\n",
    "        rng,\n",
    "        train_state,\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"train_step function defined!\")\n",
    "print()\n",
    "print(\"Decision logic:\")\n",
    "print(\n",
    "    f\"  - Buffer min fill: {config['minimum_fill_ratio']:.0%} ({int(config['level_buffer_capacity'] * config['minimum_fill_ratio'])} levels)\"\n",
    ")\n",
    "print(f\"  - Replay probability: {config['replay_prob']:.0%}\")\n",
    "print(\"  - If buffer < min fill: always DR (new levels)\")\n",
    "print(\n",
    "    \"  - If buffer >= min fill: replay with {:.0%} probability\".format(\n",
    "        config[\"replay_prob\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b767693",
   "metadata": {},
   "source": [
    "## 13. Run a Few Training Steps\n",
    "\n",
    "Let's actually run a few training steps and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661589ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT compile the train step for speed\n",
    "train_step_jit = jax.jit(train_step)\n",
    "\n",
    "# Run a few training steps\n",
    "n_steps = 5\n",
    "runner_state = (rng, train_state)\n",
    "\n",
    "print(\"Running training steps...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    runner_state, metrics = train_step_jit(runner_state, None)\n",
    "    rng, train_state = runner_state\n",
    "\n",
    "    update_type = \"DR\" if train_state.update_state == UpdateState.DR else \"REPLAY\"\n",
    "    print(f\"Step {i+1}: {update_type}\")\n",
    "    print(f\"  Buffer size: {train_state.sampler['size']}\")\n",
    "    print(f\"  DR updates: {train_state.num_dr_updates}\")\n",
    "    print(f\"  Replay updates: {train_state.num_replay_updates}\")\n",
    "    print(f\"  Mean score: {metrics['scores'].mean():.4f}\")\n",
    "    print(f\"  Mean reward: {metrics['rewards'].mean():.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a2fca",
   "metadata": {},
   "source": [
    "### 13.1 Visualize Last Generated Levels\n",
    "\n",
    "Let's see what levels are being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a688ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some levels from the last DR batch\n",
    "dr_levels = train_state.dr_last_level_batch\n",
    "n_show = min(8, config[\"num_train_envs\"])\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_show):\n",
    "    # Get the i-th level\n",
    "    level_i = jax.tree_util.tree_map(lambda x: x[i], dr_levels)\n",
    "    obs, state = env.reset_env_to_level(jax.random.PRNGKey(0), level_i, env_params)\n",
    "    img = renderer.render_state(state, env_params)\n",
    "\n",
    "    axes[i].imshow(np.array(img))\n",
    "    axes[i].set_title(f\"Level {i+1}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Last Domain Randomization Batch\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d01f0",
   "metadata": {},
   "source": [
    "## 14. Fill the Buffer and Start Replaying\n",
    "\n",
    "The buffer needs to reach minimum_fill_ratio (50%) before replay starts. Let's run more steps to fill it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1ea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many steps needed to fill buffer to minimum\n",
    "min_fill = int(config[\"level_buffer_capacity\"] * config[\"minimum_fill_ratio\"])\n",
    "levels_per_step = config[\"num_train_envs\"]\n",
    "current_size = int(train_state.sampler[\"size\"])\n",
    "steps_needed = max(\n",
    "    0, (min_fill - current_size) // levels_per_step + 10\n",
    ")  # +10 extra to ensure replay\n",
    "\n",
    "print(f\"Current buffer size: {current_size}\")\n",
    "print(f\"Minimum fill required: {min_fill}\")\n",
    "print(f\"Levels added per step: {levels_per_step}\")\n",
    "print(f\"Steps needed: {steps_needed}\")\n",
    "print()\n",
    "\n",
    "# Run the steps using jax.lax.scan for efficiency\n",
    "if steps_needed > 0:\n",
    "    print(f\"Running {steps_needed} steps to fill buffer...\")\n",
    "\n",
    "    # Use scan for efficiency\n",
    "    @jax.jit\n",
    "    def run_n_steps(runner_state, n):\n",
    "        return jax.lax.scan(train_step, runner_state, None, length=n)\n",
    "\n",
    "    runner_state, all_metrics = run_n_steps(runner_state, steps_needed)\n",
    "    rng, train_state = runner_state\n",
    "\n",
    "    print(f\"Done! Buffer size: {train_state.sampler['size']}\")\n",
    "    print(f\"Total DR updates: {train_state.num_dr_updates}\")\n",
    "    print(f\"Total Replay updates: {train_state.num_replay_updates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62b14b",
   "metadata": {},
   "source": [
    "### 14.1 Analyze the Buffer\n",
    "\n",
    "Let's look at what's in the buffer now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the buffer\n",
    "sampler = train_state.sampler\n",
    "buffer_size = int(sampler[\"size\"])\n",
    "scores = np.array(sampler[\"scores\"][:buffer_size])\n",
    "\n",
    "print(\"Buffer Statistics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Buffer size: {buffer_size}\")\n",
    "print(f\"Score range: [{scores.min():.4f}, {scores.max():.4f}]\")\n",
    "print(f\"Score mean: {scores.mean():.4f}\")\n",
    "print(f\"Score std: {scores.std():.4f}\")\n",
    "\n",
    "# Plot score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(scores, bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"Score\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distribution of Level Scores in Buffer\")\n",
    "axes[0].axvline(\n",
    "    scores.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {scores.mean():.3f}\"\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "# Sorted scores\n",
    "axes[1].plot(np.sort(scores)[::-1])\n",
    "axes[1].set_xlabel(\"Rank\")\n",
    "axes[1].set_ylabel(\"Score\")\n",
    "axes[1].set_title(\"Scores Sorted by Rank (Higher = More Learning Potential)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff071830",
   "metadata": {},
   "source": [
    "### 14.2 Visualize Highest-Scoring Levels\n",
    "\n",
    "These are the levels PLR thinks are most valuable for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of top-scoring levels\n",
    "top_k = 8\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(top_indices):\n",
    "    # Get the level at this index\n",
    "    level_i = level_sampler.get_levels(sampler, idx)\n",
    "    obs, state = env.reset_env_to_level(jax.random.PRNGKey(0), level_i, env_params)\n",
    "    img = renderer.render_state(state, env_params)\n",
    "\n",
    "    axes[i].imshow(np.array(img))\n",
    "    axes[i].set_title(f\"Score: {scores[idx]:.4f}\")\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Top 8 Highest-Scoring Levels (Most Learning Potential)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These levels have high scores, meaning:\")\n",
    "print(\"  - The agent's value estimate was higher than actual returns\")\n",
    "print(\"  - There's room for the agent to learn from these levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3772cc9f",
   "metadata": {},
   "source": [
    "## 15. Summary: The Complete PLR Pipeline\n",
    "\n",
    "Here's the complete flow:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    PLR Training Loop                             │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                  │\n",
    "│  ┌──────────────┐    Is buffer full enough?                     │\n",
    "│  │ train_step() │────────────────────────────────────┐          │\n",
    "│  └──────────────┘                                     │          │\n",
    "│         │                                             │          │\n",
    "│         ▼ No (or 20% of time)                        ▼ Yes (80%)│\n",
    "│  ┌──────────────────┐                    ┌───────────────────┐  │\n",
    "│  │  on_new_levels() │                    │ on_replay_levels()│  │\n",
    "│  │  (Domain Random) │                    │                   │  │\n",
    "│  └──────────────────┘                    └───────────────────┘  │\n",
    "│         │                                            │          │\n",
    "│         ▼                                            ▼          │\n",
    "│  1. Generate random levels              1. Sample from buffer   │\n",
    "│  2. Collect trajectories                2. Collect trajectories │\n",
    "│  3. Compute scores                      3. Update scores        │\n",
    "│  4. Add to buffer                       4. PPO update (learn!)  │\n",
    "│  5. (No PPO update in robust PLR)                               │\n",
    "│                                                                  │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Level Buffer**: Stores levels with their PLR scores\n",
    "2. **Score Function**: Measures learning potential (MaxMC or PVL)\n",
    "3. **Replay Priority**: High-scoring levels get sampled more often\n",
    "4. **Robust PLR**: Only update policy on replay levels, not DR levels\n",
    "5. **Curriculum**: Agent naturally learns on progressively harder levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31ffd1",
   "metadata": {},
   "source": [
    "## 16. Exercises to Try\n",
    "\n",
    "Now that you understand the pipeline, try these experiments:\n",
    "\n",
    "1. **Change the score function**: Set `config[\"score_function\"] = \"pvl\"` and see how scores change\n",
    "\n",
    "2. **Adjust replay probability**: Try `config[\"replay_prob\"] = 0.5` (more exploration) or `0.95` (more exploitation)\n",
    "\n",
    "3. **Enable exploratory gradients**: Set `config[\"exploratory_grad_updates\"] = True` to also learn on DR levels\n",
    "\n",
    "4. **Run longer training**: Run 100+ steps and watch the buffer evolve\n",
    "\n",
    "5. **Compare with pure DR**: Set `config[\"replay_prob\"] = 0.0` for no replay (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training for more steps\n",
    "n_more_steps = 50\n",
    "print(f\"Running {n_more_steps} more training steps...\")\n",
    "\n",
    "runner_state, all_metrics = run_n_steps(runner_state, n_more_steps)\n",
    "rng, train_state = runner_state\n",
    "\n",
    "print(f\"\\nAfter {n_more_steps} more steps:\")\n",
    "print(f\"  Buffer size: {train_state.sampler['size']}\")\n",
    "print(f\"  Total DR updates: {train_state.num_dr_updates}\")\n",
    "print(f\"  Total Replay updates: {train_state.num_replay_updates}\")\n",
    "print(\n",
    "    f\"  Replay ratio: {train_state.num_replay_updates / (train_state.num_dr_updates + train_state.num_replay_updates + 1e-8):.1%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c659244",
   "metadata": {},
   "source": [
    "## 17. Evaluate on Test Levels\n",
    "\n",
    "Let's see how well our agent performs on the evaluation levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f26ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_level(rng, train_state, level, level_name, max_steps=100):\n",
    "    \"\"\"Evaluate the trained policy on a specific level.\"\"\"\n",
    "    # Reset to level\n",
    "    obs, state = env.reset_env_to_level(rng, level, env_params)\n",
    "    hstate = PushWorldActorCritic.initialize_carry((1,))\n",
    "\n",
    "    # Run episode\n",
    "    frames = [renderer.render_state(state, env_params)]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        rng, rng_action = jax.random.split(rng)\n",
    "\n",
    "        # Get action from policy\n",
    "        obs_batch = jax.tree_util.tree_map(lambda x: x[None, None, ...], obs)\n",
    "        done_batch = jnp.array([[False]])\n",
    "        hstate, pi, _ = train_state.apply_fn(\n",
    "            train_state.params, (obs_batch, done_batch), hstate\n",
    "        )\n",
    "        action = pi.sample(seed=rng_action).squeeze()\n",
    "\n",
    "        # Step environment\n",
    "        rng, rng_step = jax.random.split(rng)\n",
    "        obs, state, reward, done, info = env.step_env(\n",
    "            rng_step, state, action, env_params\n",
    "        )\n",
    "\n",
    "        frames.append(renderer.render_state(state, env_params))\n",
    "        total_reward += float(reward)\n",
    "        step += 1\n",
    "\n",
    "    return frames, total_reward, step, done\n",
    "\n",
    "\n",
    "# Evaluate on each level\n",
    "print(\"Evaluating on test levels...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "eval_levels = config[\"eval_levels\"]\n",
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(eval_levels), figsize=(4 * len(eval_levels), 4))\n",
    "if len(eval_levels) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, level_name in enumerate(eval_levels):\n",
    "    level = Level.from_str(prefabs[level_name])\n",
    "    rng, rng_eval = jax.random.split(rng)\n",
    "    frames, reward, steps, solved = evaluate_on_level(\n",
    "        rng_eval, train_state, level, level_name\n",
    "    )\n",
    "\n",
    "    results[level_name] = {\"reward\": reward, \"steps\": steps, \"solved\": solved}\n",
    "\n",
    "    # Show final frame\n",
    "    axes[i].imshow(np.array(frames[-1]))\n",
    "    axes[i].set_title(\n",
    "        f\"{level_name}\\nR={reward:.2f}, Steps={steps}\\n{'SOLVED!' if solved else 'Failed'}\"\n",
    "    )\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "    print(f\"{level_name}: Reward={reward:.2f}, Steps={steps}, Solved={solved}\")\n",
    "\n",
    "plt.suptitle(\"Agent Performance on Evaluation Levels\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Agent has only trained for a short time!\")\n",
    "print(\"Full training (~30k updates) is needed for good performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac63b5",
   "metadata": {},
   "source": [
    "## 18. Running Full Training\n",
    "\n",
    "To run full training with all features (wandb logging, checkpointing, etc.):\n",
    "\n",
    "```bash\n",
    "# From the experiments directory:\n",
    "python pushworld_plr.py\n",
    "\n",
    "# Or with custom config:\n",
    "python pushworld_plr.py seed=123 num_updates=50000 use_wandb=True\n",
    "\n",
    "# Use ACCEL (with level mutation):\n",
    "python pushworld_plr.py ued=accel\n",
    "```\n",
    "\n",
    "The full training script includes:\n",
    "- Wandb logging with metrics and visualizations\n",
    "- Checkpointing for resuming training\n",
    "- Parallel evaluation across multiple attempts\n",
    "- Animation generation for evaluation episodes\n",
    "\n",
    "Happy training! 🎮"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
